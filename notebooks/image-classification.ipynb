{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff.federated_computation(lambda: \"Hello World!\")()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly_digits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Loads the Federated EMNIST dataset.\n",
       "\n",
       "Downloads and caches the dataset locally. If previously downloaded, tries to\n",
       "load the dataset from cache.\n",
       "\n",
       "This dataset is derived from the Leaf repository\n",
       "(https://github.com/TalwalkarLab/leaf) pre-processing of the Extended MNIST\n",
       "dataset, grouping examples by writer. Details about Leaf were published in\n",
       "\"LEAF: A Benchmark for Federated Settings\" https://arxiv.org/abs/1812.01097.\n",
       "\n",
       "*Note*: This dataset does not include some additional preprocessing that\n",
       "MNIST includes, such as size-normalization and centering.\n",
       "In the Federated EMNIST data, the value of 1.0\n",
       "corresponds to the background, and 0.0 corresponds to the color of the digits\n",
       "themselves; this is the *inverse* of some MNIST representations,\n",
       "e.g. in [tensorflow_datasets]\n",
       "(https://github.com/tensorflow/datasets/blob/master/docs/datasets.md#mnist),\n",
       "where 0 corresponds to the background color, and 255 represents the color of\n",
       "the digit.\n",
       "\n",
       "Data set sizes:\n",
       "\n",
       "*only_digits=True*: 3,383 users, 10 label classes\n",
       "\n",
       "-   train: 341,873 examples\n",
       "-   test: 40,832 examples\n",
       "\n",
       "*only_digits=False*: 3,400 users, 62 label classes\n",
       "\n",
       "-   train: 671,585 examples\n",
       "-   test: 77,483 examples\n",
       "\n",
       "Rather than holding out specific users, each user's examples are split across\n",
       "_train_ and _test_ so that all users have at least one example in _train_ and\n",
       "one example in _test_. Writers that had less than 2 examples are excluded from\n",
       "the data set.\n",
       "\n",
       "The `tf.data.Datasets` returned by\n",
       "`tff.simulation.ClientData.create_tf_dataset_for_client` will yield\n",
       "`collections.OrderedDict` objects at each iteration, with the following keys\n",
       "and values:\n",
       "\n",
       "  -   `'pixels'`: a `tf.Tensor` with `dtype=tf.float32` and shape [28, 28],\n",
       "      containing the pixels of the handwritten digit, with values in\n",
       "      the range [0.0, 1.0].\n",
       "  -   `'label'`: a `tf.Tensor` with `dtype=tf.int32` and shape [1], the class\n",
       "      label of the corresponding pixels. Labels [0-9] correspond to the digits\n",
       "      classes, labels [10-35] correspond to the uppercase classes (e.g., label\n",
       "      11 is 'B'), and labels [36-61] correspond to the lowercase classes\n",
       "      (e.g., label 37 is 'b').\n",
       "\n",
       "Args:\n",
       "  only_digits: (Optional) whether to only include examples that are from the\n",
       "    digits [0-9] classes. If `False`, includes lower and upper case\n",
       "    characters, for a total of 62 class labels.\n",
       "  cache_dir: (Optional) directory to cache the downloaded file. If `None`,\n",
       "    caches in Keras' default cache directory.\n",
       "\n",
       "Returns:\n",
       "  Tuple of (train, test) where the tuple elements are\n",
       "  `tff.simulation.ClientData` objects.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/simulation/datasets/emnist.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tff.simulation.datasets.emnist.load_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pughdr/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "emnist_train, emnist_test = (tff.simulation\n",
    "                                .datasets\n",
    "                                .emnist\n",
    "                                .load_data(only_digits=True, cache_dir=\"../data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f0000_14',\n",
       " 'f0001_41',\n",
       " 'f0005_26',\n",
       " 'f0006_12',\n",
       " 'f0008_45',\n",
       " 'f0011_13',\n",
       " 'f0014_19',\n",
       " 'f0016_39',\n",
       " 'f0017_07',\n",
       " 'f0022_10']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are what the client ids look like\n",
    "emnist_train.client_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3383"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clients = len(emnist_train.client_ids)\n",
    "n_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_random_idx = random_state.randint(n_clients)\n",
    "random_client = emnist_train.client_ids[_random_idx]\n",
    "client_dataset = emnist_train.create_tf_dataset_for_client(random_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: OrderedDict([(label, ()), (pixels, (28, 28))]), types: OrderedDict([(label, tf.int32), (pixels, tf.float32)])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is TensorFlow Dataset for a particular client\n",
    "client_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(12,6), sharex=True, sharey=True)\n",
    "for i, example in enumerate(client_dataset.take(5)):\n",
    "    axes[i].imshow(example[\"pixels\"].numpy(), cmap=\"gray\")\n",
    "    axes[i].set_title(example[\"label\"].numpy())\n",
    "_ = fig.suptitle(x= 0.5, y=0.75, t=f\"Training examples for client {random_client}\", fontsize=15)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature distribution skew(covariate shift): The marginal distributionsPi(x)may vary across clients,even ifP(y|x)is shared.4For example, in a handwriting recognition domain, users who write thesame words might still have different stroke width, slant, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each client dataset is already a `tf.data.Dataset`, preprocessing can be accomplished using Dataset transformations. If client datasets are stored in some other format then could use preprocessing operations from [`sklearn.preprocessing`](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
    "\n",
    "Here, we flatten the 28x28 images into 784-element arrays, shuffle the individual examples, organize them into batches, and renames the features from pixels and label to x and y for use with Keras. We also throw in a repeat over the data set to run several epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = (tf.data\n",
    "              .experimental\n",
    "              .AUTOTUNE)\n",
    "SHUFFLE_BUFFER_SIZE = 500\n",
    "NUMBER_TRAINING_EPOCHS = 10\n",
    "TRAINING_BATCH_SIZE = 32\n",
    "TESTING_BATCH_SIZE = 32\n",
    "\n",
    "def _reshape(training_sample):\n",
    "    \"\"\"Extracts and reshapes data from a training sample \"\"\"\n",
    "    pixels = training_sample[\"pixels\"]\n",
    "    label = training_sample[\"label\"]\n",
    "    X = tf.reshape(pixels, shape=[-1]) # flattens 2D pixels to 1D\n",
    "    y = tf.reshape(label, shape=[1])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_training_dataset(client_dataset,\n",
    "                            seed=None,\n",
    "                            num_parallel_calls=AUTOTUNE,\n",
    "                            shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "                            number_epochs=NUMBER_TRAINING_EPOCHS,\n",
    "                            batch_size=TRAINING_BATCH_SIZE,\n",
    "                            prefetch_buffer_size=AUTOTUNE):\n",
    "    \"\"\"Create a training dataset from raw client dataset.\"\"\"\n",
    "    _dataset = (client_dataset.map(_reshape, num_parallel_calls)\n",
    "                              .shuffle(shuffle_buffer_size, seed, reshuffle_each_iteration=True)\n",
    "                              .repeat(number_epochs)\n",
    "                              .batch(batch_size)\n",
    "                              .prefetch(prefetch_buffer_size))\n",
    "    return _dataset\n",
    "\n",
    "\n",
    "def create_testing_dataset(client_dataset,\n",
    "                           num_parallel_calls=AUTOTUNE,\n",
    "                           batch_size=TESTING_BATCH_SIZE):\n",
    "    \"\"\"Create a testing dataset from raw client dataset.\"\"\"\n",
    "    _dataset = (client_dataset.map(_reshape, num_parallel_calls)\n",
    "                              .batch(batch_size))\n",
    "    return _dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have almost all the building blocks in place to construct federated data sets.\n",
    "\n",
    "One of the ways to feed federated data to TFF in a simulation is simply as a Python list, with each element of the list holding the data of an individual user, whether as a list or as a tf.data.Dataset. Since we already have an interface that provides the latter, let's use it.\n",
    "\n",
    "Here's a simple helper function that will construct a list of datasets from the given set of users as an input to a round of training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_datasets(client_ids,\n",
    "                             seed,\n",
    "                             num_parallel_calls=AUTOTUNE,\n",
    "                             shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "                             number_epochs=NUMBER_TRAINING_EPOCHS,\n",
    "                             batch_size=TRAINING_BATCH_SIZE,\n",
    "                             prefetch_buffer_size=AUTOTUNE):\n",
    "    \"\"\"Creates a TF Dataset for each client id.\"\"\"\n",
    "    training_datasets = []\n",
    "    for client_id in client_ids:\n",
    "        client_dataset = emnist_train.create_tf_dataset_for_client(client_id)\n",
    "        training_dataset = (create_training_dataset(client_dataset,\n",
    "                                                    seed,\n",
    "                                                    num_parallel_calls,\n",
    "                                                    shuffle_buffer_size,\n",
    "                                                    number_epochs,\n",
    "                                                    batch_size,\n",
    "                                                    prefetch_buffer_size))\n",
    "        training_datasets.append(training_dataset)\n",
    "    return training_datasets\n",
    "\n",
    "\n",
    "def create_testing_datasets(client_ids,\n",
    "                            num_parallel_calls=AUTOTUNE,\n",
    "                            batch_size=TESTING_BATCH_SIZE):\n",
    "    \"\"\"Creates a TF Dataset for each client id.\"\"\"\n",
    "    testing_datasets = []\n",
    "    for client_id in client_ids:\n",
    "        client_dataset = emnist_test.create_tf_dataset_for_client(client_id)\n",
    "        testing_dataset = (create_testing_dataset(client_dataset,\n",
    "                                                  num_parallel_calls,\n",
    "                                                  batch_size))\n",
    "        testing_datasets.append(testing_dataset)\n",
    "    return testing_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how do we choose clients?\n",
    "\n",
    "In a typical federated training scenario, we are dealing with potentially a very large population of user devices, only a fraction of which may be available for training at a given point in time. This is the case, for example, when the client devices are mobile phones that participate in training only when plugged into a power source, off a metered network, and otherwise idle.\n",
    "\n",
    "Of course, we are in a simulation environment, and all the data is locally available. Typically then, when running simulations, we would simply sample a random subset of the clients to be involved in each round of training, generally different in each round.\n",
    "\n",
    "That said, as you can find out by studying the paper on the Federated Averaging algorithm, achieving convergence in a system with randomly sampled subsets of clients in each round can take a while, and it would be impractical to have to run hundreds of rounds in this interactive tutorial.\n",
    "\n",
    "What we'll do instead is sample the set of clients once, and reuse the same set across rounds to speed up convergence (intentionally over-fitting to these few user's data). We leave it as an exercise for the reader to modify this tutorial to simulate random sampling - it is fairly easy to do (once you do, keep in mind that getting the model to converge may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_client_ids(client_ids: list,\n",
    "                      sample_size: float,\n",
    "                      random_state: np.random.RandomState) -> list:\n",
    "    \"\"\"Randomly selects a subset of clients ids.\"\"\"\n",
    "    _size = int(sample_size * len(client_ids))\n",
    "    _random_idxs = random_state.randint(n_clients, size=_size)\n",
    "    return [client_ids[i] for i in _random_idxs]\n",
    "\n",
    "\n",
    "_random_state = np.random.RandomState(42)\n",
    "_client_ids = sample_client_ids(emnist_train.client_ids, 0.01, _random_state)\n",
    "federated_training_data = create_training_datasets(_client_ids, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(federated_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n",
       " array([[1],\n",
       "        [6],\n",
       "        [2],\n",
       "        [6],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [7],\n",
       "        [4],\n",
       "        [0],\n",
       "        [0],\n",
       "        [7],\n",
       "        [7],\n",
       "        [6],\n",
       "        [5],\n",
       "        [1],\n",
       "        [2],\n",
       "        [8],\n",
       "        [9],\n",
       "        [2],\n",
       "        [3],\n",
       "        [8],\n",
       "        [7],\n",
       "        [3],\n",
       "        [0],\n",
       "        [8],\n",
       "        [6],\n",
       "        [8],\n",
       "        [9],\n",
       "        [6],\n",
       "        [8],\n",
       "        [2]], dtype=int32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = (tf.nest\n",
    "                  .map_structure(lambda tensor: tensor.numpy(), iter(federated_training_data[0]).next()))\n",
    "\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model with Keras\n",
    "\n",
    "If you are using Keras, you likely already have code that constructs a Keras model. Here's an example of a simple model that will suffice for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeras_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdummy_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Builds a `tff.learning.Model` for an example mini batch.\n",
       "\n",
       "Args:\n",
       "  keras_model: A `tf.keras.Model` object that is not compiled.\n",
       "  dummy_batch: A nested structure of values that are convertible to *batched*\n",
       "    tensors with the same shapes and types as would be input to `keras_model`.\n",
       "    The values of the tensors are not important and can be filled with any\n",
       "    reasonable input value.\n",
       "  loss: A callable that takes two batched tensor parameters, `y_true` and\n",
       "    `y_pred`, and returns the loss. If the model has multiple outputs, you can\n",
       "    use a different loss on each output by passing a dictionary or a list of\n",
       "    losses. The loss value that will be minimized by the model will then be\n",
       "    the sum of all individual losses, each weighted by `loss_weights`.\n",
       "  loss_weights: (Optional) a list or dictionary specifying scalar coefficients\n",
       "    (Python floats) to weight the loss contributions of different model\n",
       "    outputs. The loss value that will be minimized by the model will then be\n",
       "    the *weighted sum* of all individual losses, weighted by the\n",
       "    `loss_weights` coefficients. If a list, it is expected to have a 1:1\n",
       "      mapping to the model's outputs. If a tensor, it is expected to map\n",
       "      output names (strings) to scalar coefficients.\n",
       "  metrics: (Optional) a list of `tf.keras.metrics.Metric` objects.\n",
       "  optimizer: (Optional) a `tf.keras.optimizer.Optimizer`. If None, returned\n",
       "    model cannot be used for training.\n",
       "\n",
       "Returns:\n",
       "  A `tff.learning.Model` object.\n",
       "\n",
       "Raises:\n",
       "  TypeError: If `keras_model` is not an instance of `tf.keras.Model`.\n",
       "  ValueError: If `keras_model` was compiled.\n",
       "  KeyError: If `loss` is a `dict` and does not have the same keys as\n",
       "    `keras_model.outputs`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/learning/keras_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tff.learning.from_keras_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_FEATURES = 784\n",
    "\n",
    "def create_tff_model_fn(model_fn, loss_fn, metrics, optimizer):\n",
    "    \n",
    "    def tff_model_fn():\n",
    "        dummy_batch = (tf.constant(0.0, shape=(TRAINING_BATCH_SIZE, NUMBER_FEATURES), dtype=tf.float32),\n",
    "                       tf.constant(0, shape=(TRAINING_BATCH_SIZE, 1), dtype=tf.int32))\n",
    "        tff_model = (tff.learning\n",
    "                        .from_keras_model(model_fn, dummy_batch, loss_fn, None, metrics, optimizer))\n",
    "        return tff_model\n",
    "    \n",
    "    return tff_model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_model_fn():\n",
    "    model_fn = keras.models.Sequential([\n",
    "        keras.layers.Dense(10, activation=tf.nn.softmax, kernel_initializer='zeros', input_shape=(NUMBER_FEATURES,))\n",
    "    ])\n",
    "\n",
    "    loss_fn = (keras.losses\n",
    "                    .SparseCategoricalCrossentropy())\n",
    "    optimizer = (keras.optimizers\n",
    "                      .SGD(learning_rate=0.02))\n",
    "    metrics = [\n",
    "       keras.metrics.SparseCategoricalAccuracy()\n",
    "    ]\n",
    "    \n",
    "    # optimizer only used to compute local model updates on each client\n",
    "    model_fn.compile(optimizer, loss_fn, metrics)\n",
    "    \n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "One critical note on compile. When used in the Federated Averaging algorithm, as below, the optimizer is only half of the total optimization algorithm, as it is only used to compute local model updates on each client. The rest of the algorithm involves how these updates are averaged over clients, and how they are then applied to the global model at the server. In particular, this means that the choice of optimizer and learning rate used here may need to be different than the ones you have used to train the model on a standard i.i.d. dataset. We recommend starting with regular SGD, possibly with a smaller learning rate than usual. The learning rate we use here has not been carefully tuned, feel free to experiment.\n",
    "\n",
    "One critical note on compile. When used in the Federated Averaging algorithm, as below, the optimizer is only half of the total optimization algorithm, as it is only used to compute local model updates on each client. The rest of the algorithm involves how these updates are averaged over clients, and how they are then applied to the global model at the server. In particular, this means that the choice of optimizer and learning rate used here may need to be different than the ones you have used to train the model on a standard i.i.d. dataset. We recommend starting with regular SGD, possibly with a smaller learning rate than usual. The learning rate we use here has not been carefully tuned, feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compiled_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Builds a `tff.learning.Model` for an example mini batch.\n",
       "\n",
       "Args:\n",
       "  keras_model: A `tf.keras.Model` object that was compiled.\n",
       "  dummy_batch: A nested structure of values that are convertible to *batched*\n",
       "    tensors with the same shapes and types as expected by `forward_pass()`.\n",
       "    The values of the tensors are not important and can be filled with any\n",
       "    reasonable input value.\n",
       "\n",
       "Returns:\n",
       "  A `tff.learning.Model`.\n",
       "\n",
       "Raises:\n",
       "  TypeError: If `keras_model` is not an instance of `tf.keras.Model`.\n",
       "  ValueError: If `keras_model` was *not* compiled.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/learning/keras_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tff.learning.from_compiled_keras_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tff_model_fn():\n",
    "    keras_model = keras_model_fn()\n",
    "    \n",
    "    # batch of data used to initialize parts of keras model (values irrelevant!)\n",
    "    dummy_batch = (tf.constant(0.0, shape=(TRAINING_BATCH_SIZE, NUMBER_FEATURES), dtype=tf.float32),\n",
    "                   tf.constant(0, shape=(TRAINING_BATCH_SIZE, 1), dtype=tf.int32))\n",
    "    \n",
    "    tff_model = (tff.learning\n",
    "                    .from_compiled_keras_model(keras_model, dummy_batch))\n",
    "    \n",
    "    return tff_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on federated data\n",
    "\n",
    "Now that we have a model wrapped as [`tff.learning.Model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/Model) for use with TFF, we can let TFF construct a Federated Averaging algorithm by invoking the helper function tff.learning.build_federated_averaging_process, as follows.\n",
    "\n",
    "Keep in mind that the argument needs to be a constructor (such as model_fn above), not an already-constructed instance, so that the construction of your model can happen in a context controlled by TFF (if you're curious about the reasons for this, we encourage you to read the follow-up tutorial on custom algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_federated_averaging_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mserver_optimizer_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x145f06d40\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclient_weight_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstateful_delta_aggregate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstateful_model_broadcast_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Builds the TFF computations for optimization using federated averaging.\n",
       "\n",
       "Args:\n",
       "  model_fn: A no-arg function that returns a `tff.learning.TrainableModel`.\n",
       "  server_optimizer_fn: A no-arg function that returns a `tf.Optimizer`. The\n",
       "    `apply_gradients` method of this optimizer is used to apply client updates\n",
       "    to the server model. The default creates a `tf.keras.optimizers.SGD` with\n",
       "    a learning rate of 1.0, which simply adds the average client delta to the\n",
       "    server's model.\n",
       "  client_weight_fn: Optional function that takes the output of\n",
       "    `model.report_local_outputs` and returns a tensor that provides the weight\n",
       "    in the federated average of model deltas. If not provided, the default is\n",
       "    the total number of examples processed on device.\n",
       "  stateful_delta_aggregate_fn: A `tff.utils.StatefulAggregateFn` where the\n",
       "    `next_fn` performs a federated aggregation and upates state. That is, it\n",
       "    has TFF type `(state@SERVER, value@CLIENTS, weights@CLIENTS) ->\n",
       "    (state@SERVER, aggregate@SERVER)`, where the `value` type is\n",
       "    `tff.learning.framework.ModelWeights.trainable` corresponding to the\n",
       "    object returned by `model_fn`. By default performs arithmetic mean\n",
       "    aggregation, weighted by `client_weight_fn`.\n",
       "  stateful_model_broadcast_fn: A `tff.utils.StatefulBroadcastFn` where the\n",
       "    `next_fn` performs a federated broadcast and upates state. That is, it has\n",
       "    TFF type `(state@SERVER, value@SERVER) -> (state@SERVER, value@CLIENTS)`,\n",
       "    where the `value` type is `tff.learning.framework.ModelWeights`\n",
       "    corresponding to the object returned by `model_fn`. By default performs\n",
       "    identity broadcast.\n",
       "\n",
       "Returns:\n",
       "  A `tff.utils.IterativeProcess`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/learning/federated_averaging.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tff.learning.build_federated_averaging_process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"dense_8/Identity:0\", shape=(None, 10), dtype=float32) must be from the same graph as Tensor(\"dense_8_target:0\", shape=(None, None), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-45d048b55dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m federated_averaging_process = (tff.learning\n\u001b[1;32m     18\u001b[0m                                   .build_federated_averaging_process(create_tff_model_fn(model_fn, loss_fn, metrics, optimizer),\n\u001b[0;32m---> 19\u001b[0;31m                                                                      server_optimizer_fn=_server_optimizer_fn))\n\u001b[0m",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/learning/federated_averaging.py\u001b[0m in \u001b[0;36mbuild_federated_averaging_process\u001b[0;34m(model_fn, server_optimizer_fn, client_weight_fn, stateful_delta_aggregate_fn, stateful_model_broadcast_fn)\u001b[0m\n\u001b[1;32m    165\u001b[0m   return optimizer_utils.build_model_delta_optimizer_process(\n\u001b[1;32m    166\u001b[0m       \u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_fed_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_optimizer_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       stateful_delta_aggregate_fn, stateful_model_broadcast_fn)\n\u001b[0m",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py\u001b[0m in \u001b[0;36mbuild_model_delta_optimizer_process\u001b[0;34m(model_fn, model_to_client_delta_fn, server_optimizer_fn, stateful_delta_aggregate_fn, stateful_model_broadcast_fn)\u001b[0m\n\u001b[1;32m    349\u001b[0m   \u001b[0;31m# still need this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0mdummy_model_for_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menhance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m   \u001b[0;31m# ===========================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-ebfc035d1958>\u001b[0m in \u001b[0;36mtff_model_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                        tf.constant(0, shape=(TRAINING_BATCH_SIZE, 1), dtype=tf.int32))\n\u001b[1;32m      8\u001b[0m         tff_model = (tff.learning\n\u001b[0;32m----> 9\u001b[0;31m                         .from_keras_model(model_fn, dummy_batch, loss_fn, None, metrics, optimizer))\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtff_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/learning/keras_utils.py\u001b[0m in \u001b[0;36mfrom_keras_model\u001b[0;34m(keras_model, dummy_batch, loss, loss_weights, metrics, optimizer)\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m       metrics=metrics)\n\u001b[0m\u001b[1;32m    176\u001b[0m   \u001b[0;31m# NOTE: A sub-classed tf.keras.Model does not produce the compiled metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m   \u001b[0;31m# until the model has been called on input. The work-around is to call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m           \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mskip_target_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_skip_target_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m           masks=self._prepare_output_masks())\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m       \u001b[0;31m# Prepare sample weight modes. List with the same length as model outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_handle_metrics\u001b[0;34m(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)\u001b[0m\n\u001b[1;32m   2061\u001b[0m           metric_results.extend(\n\u001b[1;32m   2062\u001b[0m               self._handle_per_output_metrics(self._per_output_metrics[i],\n\u001b[0;32m-> 2063\u001b[0;31m                                               target, output, output_mask))\n\u001b[0m\u001b[1;32m   2064\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_weighted_and_unweighted_metrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreturn_weighted_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m           metric_results.extend(\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_handle_per_output_metrics\u001b[0;34m(self, metrics_dict, y_true, y_pred, mask, weights)\u001b[0m\n\u001b[1;32m   2012\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m         metric_result = training_utils.call_metric_function(\n\u001b[0;32m-> 2014\u001b[0;31m             metric_fn, y_true, y_pred, weights=weights, mask=mask)\n\u001b[0m\u001b[1;32m   2015\u001b[0m         \u001b[0mmetric_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetric_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcall_metric_function\u001b[0;34m(metric_fn, y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m   \u001b[0;31m# `Mean` metric only takes a single value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m  \u001b[0;31m# pylint:disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     return distributed_training_utils.call_replica_local_fn(\n\u001b[0;32m--> 193\u001b[0;31m         replica_local_fn, *args, **kwargs)\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36mcall_replica_local_fn\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36mreplica_local_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplica_local_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;34m\"\"\"Updates the state of the metric in a replica-local context.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# update_op will be None in eager execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    577\u001b[0m             [y_true, y_pred], sample_weight)\n\u001b[1;32m    578\u001b[0m     y_pred, y_true = tf_losses_utils.squeeze_or_expand_dimensions(\n\u001b[0;32m--> 579\u001b[0;31m         y_pred, y_true)\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/util.py\u001b[0m in \u001b[0;36msqueeze_or_expand_dimensions\u001b[0;34m(y_pred, y_true, sample_weight)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred_rank\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_true_rank\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my_pred_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         y_true, y_pred = confusion_matrix.remove_squeezable_dimensions(\n\u001b[0;32m---> 73\u001b[0;31m             y_true, y_pred)\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0;31m# Use dynamic rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/ops/confusion_matrix.py\u001b[0m in \u001b[0;36mremove_squeezable_dimensions\u001b[0;34m(labels, predictions, expected_rank_diff, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m   \"\"\"\n\u001b[1;32m     59\u001b[0m   with ops.name_scope(name, 'remove_squeezable_dimensions',\n\u001b[0;32m---> 60\u001b[0;31m                       [labels, predictions]):\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6335\u001b[0m         \u001b[0;31m# Specialize based on the knowledge that `_get_graph_from_inputs()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6336\u001b[0m         \u001b[0;31m# ignores `inputs` when building a function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6337\u001b[0;31m         \u001b[0mg_from_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mg_from_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6339\u001b[0m           \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_from_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   5980\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5981\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5982\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5983\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5984\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   5915\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5916\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" %\n\u001b[0;32m-> 5917\u001b[0;31m                      (item, original_item))\n\u001b[0m\u001b[1;32m   5918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"dense_8/Identity:0\", shape=(None, 10), dtype=float32) must be from the same graph as Tensor(\"dense_8_target:0\", shape=(None, None), dtype=float32)."
     ]
    }
   ],
   "source": [
    "def _server_optimizer_fn():\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=1.0)\n",
    "    return optimizer\n",
    "\n",
    "model_fn = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax, kernel_initializer='zeros', input_shape=(NUMBER_FEATURES,))\n",
    "])\n",
    "\n",
    "loss_fn = (keras.losses\n",
    "                .SparseCategoricalCrossentropy())\n",
    "optimizer = (keras.optimizers\n",
    "                  .SGD(learning_rate=0.02))\n",
    "metrics = [\n",
    "   keras.metrics.SparseCategoricalAccuracy()\n",
    "]\n",
    "    \n",
    "federated_averaging_process = (tff.learning\n",
    "                                  .build_federated_averaging_process(create_tff_model_fn(model_fn, loss_fn, metrics, optimizer),\n",
    "                                                                     server_optimizer_fn=_server_optimizer_fn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "What just happened? TFF has constructed a pair of federated computations and packaged them into a tff.utils.IterativeProcess in which these computations are available as a pair of properties initialize and next.\n",
    "\n",
    "In a nutshell, federated computations are programs in TFF's internal language that can express various federated algorithms (you can find more about this in the custom algorithms tutorial). In this case, the two computations generated and packed into iterative_process implement Federated Averaging.\n",
    "\n",
    "It is a goal of TFF to define computations in a way that they could be executed in real federated learning settings, but currently only local execution simulation runtime is implemented. To execute a computation in a simulator, you simply invoke it like a Python function. This default interpreted environment is not designed for high performance, but it will suffice for this tutorial; we expect to provide higher-performance simulation runtimes to facilitate larger-scale research in future releases.\n",
    "\n",
    "Let's start with the initialize computation. As is the case for all federated computations, you can think of it as a function. The computation takes no arguments, and returns one result - the representation of the state of the Federated Averaging process on the server. While we don't want to dive into the details of TFF, it may be instructive to see what this state looks like. You can visualize it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( -> <model=<trainable=<dense/kernel=float32[784,10],dense/bias=float32[10]>,non_trainable=<>>,optimizer_state=<int64>,delta_aggregate_state=<>,model_broadcast_state=<>>@SERVER)'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(federated_averaging_process.initialize.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = federated_averaging_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 1, metrics: <sparse_categorical_accuracy=0.12496867030858994,loss=2.6366264820098877>\n"
     ]
    }
   ],
   "source": [
    "state, metrics = federated_averaging_process.next(state, federated_training_data)\n",
    "print(f\"round: 1, metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a few more rounds. As noted earlier, typically at this point you would pick a subset of your simulation data from a new randomly selected sample of users for each round in order to simulate a realistic deployment in which users continuously come and go, but in this interactive notebook, for the sake of demonstration we'll just reuse the same users, so that the system converges quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round:2, metrics:<sparse_categorical_accuracy=0.15003132820129395,loss=2.5003929138183594>\n",
      "round:3, metrics:<sparse_categorical_accuracy=0.18117168545722961,loss=2.4063901901245117>\n",
      "round:4, metrics:<sparse_categorical_accuracy=0.21218672394752502,loss=2.298753023147583>\n",
      "round:5, metrics:<sparse_categorical_accuracy=0.263627827167511,loss=2.1705288887023926>\n",
      "round:6, metrics:<sparse_categorical_accuracy=0.27666041254997253,loss=2.12200665473938>\n",
      "round:7, metrics:<sparse_categorical_accuracy=0.3250313401222229,loss=2.0114731788635254>\n",
      "round:8, metrics:<sparse_categorical_accuracy=0.35874059796333313,loss=1.9277386665344238>\n",
      "round:9, metrics:<sparse_categorical_accuracy=0.3807644248008728,loss=1.8694604635238647>\n"
     ]
    }
   ],
   "source": [
    "NUM_ROUNDS = 10\n",
    "\n",
    "for n in range(2, NUM_ROUNDS):\n",
    "    state, metrics = federated_averaging_process.next(state, federated_training_data)\n",
    "    print(f\"round:{n}, metrics:{metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing the model implementation\n",
    "\n",
    "Keras is the recommended high-level model API for TensorFlow and you should be using Keras models (via [`tff.learning.from_keras_model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/from_keras_model) or [`tff.learning.from_compiled_keras_model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/from_compiled_keras_model)) in TFF whenever possible.\n",
    "\n",
    "However, tff.learning provides a lower-level model interface, tff.learning.Model, that exposes the minimal functionality necessary for using a model for federated learning. Directly implementing this interface (possibly still using building blocks like tf.keras.layers) allows for maximum customization without modifying the internals of the federated learning algorithms.\n",
    "\n",
    "So let's do it all over again from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model variables, the forward pass, and metrics\n",
    "\n",
    "The first step is to identify the TensorFlow variables we're going to work with. In order to make the following code more legible, let's define a data structure to represent the entire set. This will include variables such as `weights` and `bias` that we will train, as well as variables that will hold various cumulative statistics and counters we will update during training, such as `loss_sum`, `accuracy_sum`, and `num_examples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "_field_names = [\n",
    "    \"weights\",\n",
    "    \"bias\",\n",
    "    \"num_examples\",\n",
    "    \"loss_sum\",\n",
    "    \"accuracy_sum\"\n",
    "]\n",
    "\n",
    "Variables = collections.namedtuple(\"Variables\", field_names=_field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "See the [Variables Guide](https://tensorflow.org/guide/variables).\n",
       "\n",
       "A variable maintains state in the graph across calls to `run()`. You add a\n",
       "variable to the graph by constructing an instance of the class `Variable`.\n",
       "\n",
       "The `Variable()` constructor requires an initial value for the variable,\n",
       "which can be a `Tensor` of any type and shape. The initial value defines the\n",
       "type and shape of the variable. After construction, the type and shape of\n",
       "the variable are fixed. The value can be changed using one of the assign\n",
       "methods.\n",
       "\n",
       "If you want to change the shape of a variable later you have to use an\n",
       "`assign` Op with `validate_shape=False`.\n",
       "\n",
       "Just like any `Tensor`, variables created with `Variable()` can be used as\n",
       "inputs for other Ops in the graph. Additionally, all the operators\n",
       "overloaded for the `Tensor` class are carried over to variables, so you can\n",
       "also add nodes to the graph by just doing arithmetic on variables.\n",
       "\n",
       "```python\n",
       "import tensorflow as tf\n",
       "\n",
       "# Create a variable.\n",
       "w = tf.Variable(<initial-value>, name=<optional-name>)\n",
       "\n",
       "# Use the variable in the graph like any Tensor.\n",
       "y = tf.matmul(w, ...another variable or tensor...)\n",
       "\n",
       "# The overloaded operators are available too.\n",
       "z = tf.sigmoid(w + y)\n",
       "\n",
       "# Assign a new value to the variable with `assign()` or a related method.\n",
       "w.assign(w + 1.0)\n",
       "w.assign_add(1.0)\n",
       "```\n",
       "\n",
       "When you launch the graph, variables have to be explicitly initialized before\n",
       "you can run Ops that use their value. You can initialize a variable by\n",
       "running its *initializer op*, restoring the variable from a save file, or\n",
       "simply running an `assign` Op that assigns a value to the variable. In fact,\n",
       "the variable *initializer op* is just an `assign` Op that assigns the\n",
       "variable's initial value to the variable itself.\n",
       "\n",
       "```python\n",
       "# Launch the graph in a session.\n",
       "with tf.compat.v1.Session() as sess:\n",
       "    # Run the variable initializer.\n",
       "    sess.run(w.initializer)\n",
       "    # ...you now can run ops that use the value of 'w'...\n",
       "```\n",
       "\n",
       "The most common initialization pattern is to use the convenience function\n",
       "`global_variables_initializer()` to add an Op to the graph that initializes\n",
       "all the variables. You then run that Op after launching the graph.\n",
       "\n",
       "```python\n",
       "# Add an Op to initialize global variables.\n",
       "init_op = tf.compat.v1.global_variables_initializer()\n",
       "\n",
       "# Launch the graph in a session.\n",
       "with tf.compat.v1.Session() as sess:\n",
       "    # Run the Op that initializes global variables.\n",
       "    sess.run(init_op)\n",
       "    # ...you can now run any Op that uses variable values...\n",
       "```\n",
       "\n",
       "If you need to create a variable with an initial value dependent on another\n",
       "variable, use the other variable's `initialized_value()`. This ensures that\n",
       "variables are initialized in the right order.\n",
       "\n",
       "All variables are automatically collected in the graph where they are\n",
       "created. By default, the constructor adds the new variable to the graph\n",
       "collection `GraphKeys.GLOBAL_VARIABLES`. The convenience function\n",
       "`global_variables()` returns the contents of that collection.\n",
       "\n",
       "When building a machine learning model it is often convenient to distinguish\n",
       "between variables holding the trainable model parameters and other variables\n",
       "such as a `global step` variable used to count training steps. To make this\n",
       "easier, the variable constructor supports a `trainable=<bool>` parameter. If\n",
       "`True`, the new variable is also added to the graph collection\n",
       "`GraphKeys.TRAINABLE_VARIABLES`. The convenience function\n",
       "`trainable_variables()` returns the contents of this collection. The\n",
       "various `Optimizer` classes use this collection as the default list of\n",
       "variables to optimize.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Creates a new variable with value `initial_value`.\n",
       "\n",
       "The new variable is added to the graph collections listed in `collections`,\n",
       "which defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n",
       "\n",
       "If `trainable` is `True` the variable is also added to the graph collection\n",
       "`GraphKeys.TRAINABLE_VARIABLES`.\n",
       "\n",
       "This constructor creates both a `variable` Op and an `assign` Op to set the\n",
       "variable to its initial value.\n",
       "\n",
       "Args:\n",
       "  initial_value: A `Tensor`, or Python object convertible to a `Tensor`,\n",
       "    which is the initial value for the Variable. The initial value must have\n",
       "    a shape specified unless `validate_shape` is set to False. Can also be a\n",
       "    callable with no argument that returns the initial value when called. In\n",
       "    that case, `dtype` must be specified. (Note that initializer functions\n",
       "    from init_ops.py must first be bound to a shape before being used here.)\n",
       "  trainable: If `True`, GradientTapes automatically watch uses of this\n",
       "    variable. Defaults to `True`, unless `synchronization` is set to\n",
       "    `ON_READ`, in which case it defaults to `False`.\n",
       "  validate_shape: If `False`, allows the variable to be initialized with a\n",
       "    value of unknown shape. If `True`, the default, the shape of\n",
       "    `initial_value` must be known.\n",
       "  caching_device: Optional device string describing where the Variable\n",
       "    should be cached for reading.  Defaults to the Variable's device. If not\n",
       "    `None`, caches on another device.  Typical use is to cache on the device\n",
       "    where the Ops using the Variable reside, to deduplicate copying through\n",
       "    `Switch` and other conditional statements.\n",
       "  name: Optional name for the variable. Defaults to `'Variable'` and gets\n",
       "    uniquified automatically.\n",
       "  variable_def: `VariableDef` protocol buffer. If not `None`, recreates the\n",
       "    Variable object with its contents, referencing the variable's nodes in\n",
       "    the graph, which must already exist. The graph is not changed.\n",
       "    `variable_def` and the other arguments are mutually exclusive.\n",
       "  dtype: If set, initial_value will be converted to the given type. If\n",
       "    `None`, either the datatype will be kept (if `initial_value` is a\n",
       "    Tensor), or `convert_to_tensor` will decide.\n",
       "  import_scope: Optional `string`. Name scope to add to the `Variable.` Only\n",
       "    used when initializing from protocol buffer.\n",
       "  constraint: An optional projection function to be applied to the variable\n",
       "    after being updated by an `Optimizer` (e.g. used to implement norm\n",
       "    constraints or value constraints for layer weights). The function must\n",
       "    take as input the unprojected Tensor representing the value of the\n",
       "    variable and return the Tensor for the projected value (which must have\n",
       "    the same shape). Constraints are not safe to use when doing asynchronous\n",
       "    distributed training.\n",
       "  synchronization: Indicates when a distributed a variable will be\n",
       "    aggregated. Accepted values are constants defined in the class\n",
       "    `tf.VariableSynchronization`. By default the synchronization is set to\n",
       "    `AUTO` and the current `DistributionStrategy` chooses when to\n",
       "    synchronize.\n",
       "  aggregation: Indicates how a distributed variable will be aggregated.\n",
       "    Accepted values are constants defined in the class\n",
       "    `tf.VariableAggregation`.\n",
       "  shape: (optional) The shape of this variable. If None, the shape of\n",
       "    `initial_value` will be used. When setting this argument to\n",
       "    `tf.TensorShape(None)` (representing an unspecified shape), the variable\n",
       "    can be assigned with values of different shapes.\n",
       "\n",
       "Raises:\n",
       "  ValueError: If both `variable_def` and initial_value are specified.\n",
       "  ValueError: If the initial value is not specified, or does not have a\n",
       "    shape and `validate_shape` is `True`.\n",
       "  RuntimeError: If eager execution is enabled.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\n",
       "\u001b[0;31mType:\u001b[0m           VariableMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     VariableV1, AbstractVariable, AggregatingVariable\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.Variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_variables():\n",
    "    _weights = tf.Variable(\n",
    "        initial_value=lambda: tf.zeros(dtype=tf.float32, shape=(784, 10)),\n",
    "        name='weights',\n",
    "        trainable=True\n",
    "    )\n",
    "    _bias = tf.Variable(\n",
    "        initial_value=lambda: tf.zeros(dtype=tf.float32, shape=(10)),\n",
    "        name='bias',\n",
    "        trainable=True\n",
    "    )\n",
    "    _num_examples = tf.Variable(0.0, name='num_examples', trainable=False)\n",
    "    _loss_sum = tf.Variable(0.0, name='loss_sum', trainable=False)\n",
    "    _accuracy_sum = tf.Variable(0.0, name='accuracy_sum', trainable=False)\n",
    "    return Variables(_weights, _bias, _num_examples, _loss_sum, _accuracy_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the variables for model parameters and cumulative statistics in place, we can now define the forward pass method that computes loss, emits predictions, and updates the cumulative statistics for a single batch of input data, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_forward_pass(variables, batch):\n",
    "    Z = tf.matmul(batch['X'], variables.weights) + variables.bias\n",
    "    y = tf.nn.softmax(Z)\n",
    "    predictions = tf.argmax(y, axis=1, output_type=tf.int32)\n",
    "\n",
    "    flat_labels = tf.reshape(batch['y'], [-1])\n",
    "    loss = -tf.reduce_mean(tf.reduce_sum(\n",
    "        tf.one_hot(flat_labels, 10) * tf.math.log(y), axis=[1]))\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(predictions, flat_labels), tf.float32))\n",
    "\n",
    "    num_examples = tf.cast(tf.size(batch['y']), tf.float32)\n",
    "\n",
    "    variables.num_examples.assign_add(num_examples)\n",
    "    variables.loss_sum.assign_add(loss * num_examples)\n",
    "    variables.accuracy_sum.assign_add(accuracy * num_examples)\n",
    "\n",
    "    return loss, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function that returns a set of local metrics. These are the values, in addition to model updates (which are handled automatically), that are eligible to be aggregated to the server in a federated learning or evaluation process.\n",
    "\n",
    "Here, we simply return the average loss and accuracy, as well as the num_examples, which we'll need to correctly weight the contributions from different users when computing federated aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_mnist_metrics(variables):\n",
    "    metrics = collections.OrderedDict([\n",
    "        ('num_examples', variables.num_examples),\n",
    "        ('loss', variables.loss_sum / variables.num_examples),\n",
    "        ('accuracy', variables.accuracy_sum / variables.num_examples)\n",
    "      ])\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to determine how to aggregate the local metrics emitted by each device via get_local_mnist_metrics. This is the only part of the code that isn't written in TensorFlow - it's a federated computation expressed in TFF. If you'd like to dig deeper, skim over the custom algorithms tutorial, but in most applications, you won't really need to; variants of the pattern shown below should suffice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tff.federated_computation\n",
    "def aggregate_mnist_metrics_across_clients(metrics):\n",
    "    aggregated_metrics = {\n",
    "        'num_examples': tff.federated_sum(metrics.num_examples),\n",
    "        'loss': tff.federated_mean(metrics.loss, metrics.num_examples),\n",
    "        'accuracy': tff.federated_mean(metrics.accuracy, metrics.num_examples)\n",
    "    }\n",
    "    return aggregated_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input metrics argument corresponds to the OrderedDict returned by get_local_mnist_metrics above, but critically the values are no longer tf.Tensors - they are \"boxed\" as tff.Values, to make it clear you can no longer manipulate them using TensorFlow, but only using TFF's federated operators like tff.federated_mean and tff.federated_sum. The returned dictionary of global aggregates defines the set of metrics which will be available on the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an instance of `tff.learning.Model`\n",
    "\n",
    "With all of the above in place, we are ready to construct a model representation for use with TFF similar to one that's generated for you when you let TFF ingest a Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tff.learning.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._variables = create_mnist_variables()\n",
    "        \n",
    "        self._input_spec = collections.OrderedDict([\n",
    "            ('X', tf.TensorSpec([None, 784], tf.float32)),\n",
    "            ('y', tf.TensorSpec([None, 1], tf.int32))\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return [self._variables.weights, self._variables.bias]\n",
    "\n",
    "    @property\n",
    "    def non_trainable_variables(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def local_variables(self):\n",
    "        _local_variables = [\n",
    "            self._variables.num_examples,\n",
    "            self._variables.loss_sum,\n",
    "            self._variables.accuracy_sum\n",
    "        ]\n",
    "        return _local_variables\n",
    "\n",
    "    @property\n",
    "    def input_spec(self):\n",
    "        return self._input_spec\n",
    "\n",
    "    @tf.function\n",
    "    def forward_pass(self, batch, training=True):\n",
    "        del training # WTF!\n",
    "        loss, predictions = mnist_forward_pass(self._variables, batch)\n",
    "        num_examples = tf.shape(batch['X'])[0]\n",
    "        \n",
    "        batch_output = tff.learning.BatchOutput(\n",
    "            loss=loss,\n",
    "            predictions=predictions,\n",
    "            num_examples=num_examples\n",
    "        )\n",
    "        return batch_output\n",
    "\n",
    "    @tf.function\n",
    "    def report_local_outputs(self):\n",
    "        return get_local_mnist_metrics(self._variables)\n",
    "\n",
    "    @property\n",
    "    def federated_output_computation(self):\n",
    "        return aggregate_mnist_metrics_across_clients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the abstract methods and properties defined by [`tff.learning.Model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/Model) corresponds to the code snippets in the preceding section that introduced the variables and defined the loss and statistics.\n",
    "\n",
    "Here are a few points worth highlighting:\n",
    "\n",
    "* All state that your model will use must be captured as TensorFlow variables, as TFF does not use Python at runtime (remember your code should be written such that it can be deployed to mobile devices).\n",
    "* Your model should describe what form of data it accepts (input_spec), as in general, TFF is a strongly-typed environment and wants to determine type signatures for all components. Declaring the format of your model's input is an essential part of it.\n",
    "* Although technically not required, we recommend wrapping all TensorFlow logic (forward pass, metric calculations, etc.) as tf.functions, as this helps ensure the TensorFlow can be serialized, and removes the need for explicit control dependencies.\n",
    "\n",
    "The above is sufficient for evaluation and algorithms like Federated SGD. However, for Federated Averaging, we need to specify how the model should train locally on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTrainableModel(MNISTModel, tff.learning.TrainableModel):\n",
    "\n",
    "    @tf.function\n",
    "    def train_on_batch(self, batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self.forward_pass(batch)\n",
    "        grads = tape.gradient(output.loss, self.trainable_variables)\n",
    "        optimizer = tf.keras.optimizers.SGD(0.02) # don't hard code this here!\n",
    "        optimizer.apply_gradients(zip(tf.nest.flatten(grads), tf.nest.flatten(self.trainable_variables)))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating federated training with the new model\n",
    "\n",
    "With all the above in place, the remainder of the process looks like what we've seen already - just replace the model constructor with the constructor of our new model class, and use the two federated computations in the iterative process you created to cycle through training rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x146184ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x146184ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "round:0, metrics:<accuracy=0.1288272738456726,loss=2.624279499053955,num_examples=34620.0>\n",
      "round:1, metrics:<accuracy=0.15038615465164185,loss=2.513991594314575,num_examples=32370.0>\n",
      "round:2, metrics:<accuracy=0.19206111133098602,loss=2.393942356109619,num_examples=33380.0>\n",
      "round:3, metrics:<accuracy=0.23060978949069977,loss=2.2518303394317627,num_examples=33290.0>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-4865e1df0378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# resample 1% of all clients at each round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mclient_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_client_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memnist_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfederated_training_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_training_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# perform the federated computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-971d00fb49d2>\u001b[0m in \u001b[0;36mcreate_training_datasets\u001b[0;34m(client_ids, seed, num_parallel_calls, shuffle_buffer_size, number_epochs, batch_size, prefetch_buffer_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                     \u001b[0mnumber_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                                     prefetch_buffer_size))\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtraining_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-025824f690ab>\u001b[0m in \u001b[0;36mcreate_training_dataset\u001b[0;34m(client_dataset, seed, num_parallel_calls, shuffle_buffer_size, number_epochs, batch_size, prefetch_buffer_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m                             prefetch_buffer_size=AUTOTUNE):\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m\"\"\"Create a training dataset from raw client dataset.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     _dataset = (client_dataset.map(_reshape, num_parallel_calls)\n\u001b[0m\u001b[1;32m     27\u001b[0m                               \u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle_buffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshuffle_each_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                               \u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m       return ParallelMapDataset(\n\u001b[0;32m-> 1214\u001b[0;31m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3452\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3453\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3454\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3455\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[1;32m   3456\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m       \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1852\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 1854\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   1855\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mkwarg_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     func_args = _get_defun_inputs_from_args(\n\u001b[0;32m--> 837\u001b[0;31m         args, arg_names, flat_shapes=arg_shapes)\n\u001b[0m\u001b[1;32m    838\u001b[0m     func_kwargs = _get_defun_inputs_from_kwargs(\n\u001b[1;32m    839\u001b[0m         kwargs, flat_shapes=kwarg_shapes)\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs_from_args\u001b[0;34m(args, names, flat_shapes)\u001b[0m\n\u001b[1;32m   1060\u001b[0m   \u001b[0;34m\"\"\"Maps Python function positional args to graph-construction inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m   return _get_defun_inputs(\n\u001b[0;32m-> 1062\u001b[0;31m       args, names, structure=args, flat_shapes=flat_shapes)\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs\u001b[0;34m(args, names, structure, flat_shapes)\u001b[0m\n\u001b[1;32m   1128\u001b[0m           placeholder = graph_placeholder(\n\u001b[1;32m   1129\u001b[0m               \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m               name=requested_name)\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m           \u001b[0;31m# Sometimes parameter names are not valid op names, so fall back to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/graph_only_ops.py\u001b[0m in \u001b[0;36mgraph_placeholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"placeholder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     op = g.create_op(\"Placeholder\", [], [dtype], input_types=[],\n\u001b[0;32m---> 51\u001b[0;31m                      attrs={\"dtype\": dtype_value, \"shape\": shape}, name=name)\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    546\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    547\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3427\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3428\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3429\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3430\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1771\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1772\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1773\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1774\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "federated_averaging_process = (tff.learning\n",
    "                                  .build_federated_averaging_process(MNISTrainableModel))\n",
    "state = federated_averaging_process.initialize()\n",
    "\n",
    "for n in range(NUM_ROUNDS):\n",
    "    \n",
    "    # resample 1% of all clients at each round\n",
    "    client_ids = sample_client_ids(emnist_train.client_ids, 0.01, random_state)\n",
    "    federated_training_data = create_training_datasets(client_ids, seed=n)\n",
    "\n",
    "    # perform the federated computation\n",
    "    state, metrics = federated_averaging_process.next(state, federated_training_data)\n",
    "    print(f\"round:{n}, metrics:{metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "All of our experiments so far presented only federated training metrics - the average metrics over all batches of data trained across all clients in the round. This introduces the normal concerns about overfitting, especially since we used the same set of clients on each round for simplicity, but there is an additional notion of overfitting in training metrics specific to the Federated Averaging algorithm. This is easiest to see if we imagine each client had a single batch of data, and we train on that batch for many iterations (epochs). In this case, the local model will quickly exactly fit to that one batch, and so the local accuracy metric we average will approach 1.0. Thus, these training metrics can be taken as a sign that training is progressing, but not much more.\n",
    "\n",
    "To perform evaluation on federated data, you can construct another federated computation designed for just this purpose, using the tff.learning.build_federated_evaluation function, and passing in your model constructor as an argument. Note that unlike with Federated Averaging, where we've used MnistTrainableModel, it suffices to pass the MnistModel. Evaluation doesn't perform gradient descent, and there's no need to construct optimizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = (tff.learning\n",
    "                 .build_federated_evaluation(MNISTModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<<trainable=<weights=float32[784,10],bias=float32[10]>,non_trainable=<>>@SERVER,{<X=float32[?,784],y=int32[?,1]>*}@CLIENTS> -> <accuracy=float32@SERVER,loss=float32@SERVER,num_examples=float32@SERVER>)\n"
     ]
    }
   ],
   "source": [
    "print(evaluation.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SERVER_MODEL, FEDERATED_DATA -> TRAINING_METRICS`\n",
    "\n",
    "No need to be concerned about the details at this point, just be aware that it takes the following general form, similar to tff.utils.IterativeProcess.next but with two important differences. \n",
    "\n",
    "1. We are not returning server state, since evaluation doesn't modify the model or any other aspect of state - you can think of it as stateless.\n",
    "2. Evaluation only needs the model and doesn't require any other part of server state that might be associated with training, such as optimizer variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics = evaluation(state.model, federated_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnonymousTuple([('accuracy', 0.59687597), ('loss', 1.8894539), ('num_examples', 33290.0)])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we get. Note the numbers look marginally better than what was reported by the last round of training above. By convention, the training metrics reported by the iterative training process generally reflect the performance of the model at the beginning of the training round, so the evaluation metrics will always be one step ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pughdr/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "# resample 1% of all clients\n",
    "client_ids = sample_client_ids(emnist_test.client_ids, 0.01, random_state)\n",
    "federated_testing_data = create_testing_datasets(client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = evaluation(state.model, federated_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnonymousTuple([('accuracy', 0.6329114), ('loss', 1.8689167), ('num_examples', 395.0)])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to play with the parameters (e.g., batch sizes, number of users, epochs, learning rates, etc.), to modify the code above to simulate training on random samples of users in each round, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a proper training loop that resamples the client ids to determine which client's datasets will be used for training within a round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_averaging_process = (tff.learning\n",
    "                                  .build_federated_averaging_process(tff_model_fn))\n",
    "state = federated_averaging_process.initialize()\n",
    "\n",
    "for n in range(NUM_ROUNDS):\n",
    "    \n",
    "    # resample 1% of all clients at each round\n",
    "    client_ids = sample_client_ids(emnist_train.client_ids, 0.01, random_state)\n",
    "    federated_training_data = create_training_datasets(client_ids)\n",
    "\n",
    "    # perform the federated computation\n",
    "    state, metrics = federated_averaging_process.next(state, federated_training_data)\n",
    "    print(f\"round:{n}, metrics:{metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pughdr/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "federated_evaluation = (tff.learning\n",
    "                           .build_federated_evaluation(tff_model_fn))\n",
    "\n",
    "# create a test set using 1% of all clients\n",
    "_client_ids = sample_client_ids(emnist_test.client_ids, 0.01, random_state)\n",
    "federated_testing_data = create_testing_datasets(_client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
