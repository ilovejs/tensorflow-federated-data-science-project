{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required to run TFF inside Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use the classic MNIST dataset to introduce the Federated Learning (FL) API layer of TFF, [`tff.learning`](https://www.tensorflow.org/federated/api_docs/python/tff/learning) - a set of high-level interfaces that can be used to perform common types of federated learning tasks, such as federated training, against user-supplied models implemented in TensorFlow or Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Hello, World!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "\n",
    "TODO: Define the difference between IID and non-IID data.\n",
    "\n",
    "With IID data federated learning doesn't seem any different than standard distributed training.\n",
    "Discuss Horovod example.\n",
    "\n",
    "Federated learning requires a federated data set, i.e., a collection of data from multiple users. Federated data is typically non-i.i.d., which poses a unique set of challenges.\n",
    "\n",
    "In order to facilitate experimentation TFF includes federated versions of several populare datasets, including a federated version of MNIST that contains a version of the original NIST dataset that has been re-processed using Leaf so that the data is keyed by the original writer of the digits. Since each writer has a unique style, this dataset exhibits the kind of non-i.i.d. behavior expected of federated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What datasets are available?\n",
    "tff.simulation.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly_digits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Loads the Federated EMNIST dataset.\n",
       "\n",
       "Downloads and caches the dataset locally. If previously downloaded, tries to\n",
       "load the dataset from cache.\n",
       "\n",
       "This dataset is derived from the Leaf repository\n",
       "(https://github.com/TalwalkarLab/leaf) pre-processing of the Extended MNIST\n",
       "dataset, grouping examples by writer. Details about Leaf were published in\n",
       "\"LEAF: A Benchmark for Federated Settings\" https://arxiv.org/abs/1812.01097.\n",
       "\n",
       "*Note*: This dataset does not include some additional preprocessing that\n",
       "MNIST includes, such as size-normalization and centering.\n",
       "In the Federated EMNIST data, the value of 1.0\n",
       "corresponds to the background, and 0.0 corresponds to the color of the digits\n",
       "themselves; this is the *inverse* of some MNIST representations,\n",
       "e.g. in [tensorflow_datasets]\n",
       "(https://github.com/tensorflow/datasets/blob/master/docs/datasets.md#mnist),\n",
       "where 0 corresponds to the background color, and 255 represents the color of\n",
       "the digit.\n",
       "\n",
       "Data set sizes:\n",
       "\n",
       "*only_digits=True*: 3,383 users, 10 label classes\n",
       "\n",
       "-   train: 341,873 examples\n",
       "-   test: 40,832 examples\n",
       "\n",
       "*only_digits=False*: 3,400 users, 62 label classes\n",
       "\n",
       "-   train: 671,585 examples\n",
       "-   test: 77,483 examples\n",
       "\n",
       "Rather than holding out specific users, each user's examples are split across\n",
       "_train_ and _test_ so that all users have at least one example in _train_ and\n",
       "one example in _test_. Writers that had less than 2 examples are excluded from\n",
       "the data set.\n",
       "\n",
       "The `tf.data.Datasets` returned by\n",
       "`tff.simulation.ClientData.create_tf_dataset_for_client` will yield\n",
       "`collections.OrderedDict` objects at each iteration, with the following keys\n",
       "and values:\n",
       "\n",
       "  -   `'pixels'`: a `tf.Tensor` with `dtype=tf.float32` and shape [28, 28],\n",
       "      containing the pixels of the handwritten digit, with values in\n",
       "      the range [0.0, 1.0].\n",
       "  -   `'label'`: a `tf.Tensor` with `dtype=tf.int32` and shape [1], the class\n",
       "      label of the corresponding pixels. Labels [0-9] correspond to the digits\n",
       "      classes, labels [10-35] correspond to the uppercase classes (e.g., label\n",
       "      11 is 'B'), and labels [36-61] correspond to the lowercase classes\n",
       "      (e.g., label 37 is 'b').\n",
       "\n",
       "Args:\n",
       "  only_digits: (Optional) whether to only include examples that are from the\n",
       "    digits [0-9] classes. If `False`, includes lower and upper case\n",
       "    characters, for a total of 62 class labels.\n",
       "  cache_dir: (Optional) directory to cache the downloaded file. If `None`,\n",
       "    caches in Keras' default cache directory.\n",
       "\n",
       "Returns:\n",
       "  Tuple of (train, test) where the tuple elements are\n",
       "  `tff.simulation.ClientData` objects.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/simulation/datasets/emnist.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tff.simulation.datasets.emnist.load_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pughdr/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:64: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "emnist_train, emnist_test = (tff.simulation\n",
    "                                .datasets\n",
    "                                .emnist\n",
    "                                .load_data(only_digits=True, cache_dir=\"../data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3383"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_CLIENTS = len(emnist_train.client_ids)\n",
    "NUMBER_CLIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_client_ids(client_ids: typing.List[str],\n",
    "                      sample_size: typing.Union[float, int],\n",
    "                      random_state: np.random.RandomState) -> typing.List[str]:\n",
    "    \"\"\"Randomly selects a subset of clients ids.\"\"\"\n",
    "    number_clients = len(client_ids)\n",
    "    error_msg = \"'client_ids' must be non-emtpy.\"\n",
    "    assert number_clients > 0, error_msg\n",
    "    if isinstance(sample_size, float):\n",
    "        error_msg = \"Sample size must be between 0 and 1.\"\n",
    "        assert 0 <= sample_size <= 1, error_msg\n",
    "        size = int(sample_size * number_clients)\n",
    "    elif isinstance(sample_size, int):\n",
    "        error_msg = f\"Sample size must be between 0 and {number_clients}.\"\n",
    "        assert 0 <= sample_size <= number_clients, error_msg\n",
    "        size = sample_size\n",
    "    else:\n",
    "        error_msg = \"Type of 'sample_size' must be 'float' or 'int'.\"\n",
    "        raise TypeError(error_msg)\n",
    "    random_idxs = random_state.randint(number_clients, size=size)\n",
    "    return [client_ids[i] for i in random_idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f3891_31',\n",
       " 'f1059_15',\n",
       " 'f1496_39',\n",
       " 'f1332_24',\n",
       " 'f1297_30',\n",
       " 'f3809_17',\n",
       " 'f1846_45',\n",
       " 'f2385_83',\n",
       " 'f0663_30',\n",
       " 'f1440_10']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are what the client ids look like\n",
    "_random_state = np.random.RandomState(42)\n",
    "sample_client_ids(emnist_train.client_ids, 10, _random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_datasets(source: tff.simulation.ClientData,\n",
    "                           client_ids: typing.List[str]) -> typing.Dict[str, tf.data.Dataset]:\n",
    "    datasets = {client_id: source.create_tf_dataset_for_client(client_id) for client_id in client_ids}\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def sample_client_datasets(source: tff.simulation.ClientData,\n",
    "                           sample_size: typing.Union[float, int],\n",
    "                           random_state: np.random.RandomState) -> typing.List[tf.data.Dataset]:\n",
    "    \"\"\"Randomly selects a subset of client datasets.\"\"\"\n",
    "    client_ids = sample_client_ids(source.client_ids, sample_size, random_state)\n",
    "    client_datasets = create_client_datasets(source, client_ids)\n",
    "    return client_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_random_state = np.random.RandomState(42)\n",
    "client_datasets = sample_client_datasets(emnist_train, sample_size=1, random_state=_random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is TensorFlow Dataset for a particular client\n",
    "(client_id, client_dataset), *_ = client_datasets.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(12,6), sharex=True, sharey=True)\n",
    "for i, example in enumerate(client_dataset.take(5)):\n",
    "    axes[i].imshow(example[\"pixels\"].numpy(), cmap=\"gray\")\n",
    "    axes[i].set_title(example[\"label\"].numpy())\n",
    "_ = fig.suptitle(x= 0.5, y=0.75, t=f\"Training examples for a client {client_id}\", fontsize=15)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature distribution skew(covariate shift): The marginal distributionsPi(x)may vary across clients,even ifP(y|x)is shared.4For example, in a handwriting recognition domain, users who write thesame words might still have different stroke width, slant, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Since each client dataset is already a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), preprocessing can be accomplished using Dataset transformations. Another option would be to use preprocessing operations from [`sklearn.preprocessing`](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
    "\n",
    "Preprocessing consists of the following steps:\n",
    "\n",
    "1. `map` a function that flattens the 28 x 28 images into 784-element tensors\n",
    "2. `map` a function that rename the features from pixels and label to X and y for use with Keras\n",
    "3. `shuffle` the individual examples\n",
    "4. `batch` the into training batches\n",
    "\n",
    "We also throw in a `repeat` over the data set to run several epochs on each client device before sending parameters to the server for averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = (tf.data\n",
    "              .experimental\n",
    "              .AUTOTUNE)\n",
    "SHUFFLE_BUFFER_SIZE = 500\n",
    "NUMBER_TRAINING_EPOCHS = 1 # number of local updates!\n",
    "TRAINING_BATCH_SIZE = 32\n",
    "TESTING_BATCH_SIZE = 32\n",
    "\n",
    "NUMBER_FEATURES = 28 * 28\n",
    "NUMBER_TARGETS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape(training_batch):\n",
    "    \"\"\"Extracts and reshapes data from a training sample \"\"\"\n",
    "    pixels = training_batch[\"pixels\"]\n",
    "    label = training_batch[\"label\"]\n",
    "    X = tf.reshape(pixels, shape=[-1]) # flattens 2D pixels to 1D\n",
    "    y = tf.reshape(label, shape=[1])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_training_dataset(client_dataset: tf.data.Dataset,\n",
    "                            seed: typing.Union[None, int] = None,\n",
    "                            shuffle_buffer_size: int = SHUFFLE_BUFFER_SIZE,\n",
    "                            number_epochs: int = NUMBER_TRAINING_EPOCHS,\n",
    "                            batch_size: int = TRAINING_BATCH_SIZE) -> tf.data.Dataset:\n",
    "    \"\"\"Create a training dataset for a client from a raw client dataset.\"\"\"\n",
    "    training_dataset = (client_dataset.map(_reshape, num_parallel_calls=AUTOTUNE)\n",
    "                                      .shuffle(shuffle_buffer_size, seed, reshuffle_each_iteration=True)\n",
    "                                      .repeat(number_epochs)\n",
    "                                      .batch(batch_size)\n",
    "                                      .prefetch(buffer_size=AUTOTUNE))\n",
    "    return training_dataset\n",
    "\n",
    "\n",
    "def create_testing_dataset(client_dataset: tf.data.Dataset,\n",
    "                           batch_size: int = TESTING_BATCH_SIZE) -> tf.data.Dataset:\n",
    "    \"\"\"Create a testing dataset for a client from a raw client dataset.\"\"\"\n",
    "    testing_dataset = (client_dataset.map(_reshape, num_parallel_calls=AUTOTUNE)\n",
    "                                     .batch(batch_size))\n",
    "    return testing_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose the clients included in each training round\n",
    "\n",
    "In a typical federated training scenario there will be a very large population of user devices however only a fraction of these devices are likely to be available for training at a given point in time. For example, if the client devices are mobile phones then they might only participate in training when plugged into a power source, off a metered network, and otherwise idle.\n",
    "\n",
    "In a simulated environment, where all data is locally available, an approach is to simply sample a random subset of the clients to be involved in each round of training so that the subset of clients involved will vary from round to round.\n",
    "\n",
    "### How many clients to include in each round?\n",
    "\n",
    "Updating and averaging a larger number of client models per training round yields better convergence and in a simulated training environment probably makes sense to include as many clients as is computationally feasible. However in real-world training scenario while averaging a larger number of clients improve convergence, it also makes training vulnerable to slowdown due to unpredictable tail delays in computation/communication at/with the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_federated_data(training_source: tff.simulation.ClientData,\n",
    "                          testing_source: tff.simulation.ClientData,\n",
    "                          sample_size: typing.Union[float, int],\n",
    "                          random_state: np.random.RandomState) -> typing.Dict[str, typing.Tuple[tf.data.Dataset, tf.data.Dataset]]:\n",
    "    # sample clients ids from the training dataset\n",
    "    client_ids = sample_client_ids(training_source.client_ids, sample_size, random_state)\n",
    "    \n",
    "    federated_data = {}\n",
    "    for client_id in client_ids:\n",
    "        # create training dataset for the client\n",
    "        _tf_dataset = training_source.create_tf_dataset_for_client(client_id)\n",
    "        training_dataset = create_training_dataset(_tf_dataset)\n",
    "        \n",
    "        # create the testing dataset for the client\n",
    "        _tf_dataset = testing_source.create_tf_dataset_for_client(client_id)\n",
    "        testing_dataset = create_testing_dataset(_tf_dataset)\n",
    "        \n",
    "        federated_data[client_id] = (training_dataset, testing_dataset)\n",
    "    \n",
    "    return federated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pughdr/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:64: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "_random_state = np.random.RandomState(42)\n",
    "federated_data = create_federated_data(emnist_train,\n",
    "                                       emnist_test,\n",
    "                                       sample_size=0.01,\n",
    "                                       random_state=_random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys are client ids, values are (training_dataset, testing_dataset) pairs\n",
    "len(federated_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model with Keras\n",
    "\n",
    "If you are using Keras, you likely already have code that constructs a Keras model. Since the model will need to be replicated on each of the client devices we wrap the model in a no-argument Python function, a representation of which, will eventually be invoked on each client to create the model on that client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model_fn() -> keras.Model:\n",
    "    model_fn = keras.models.Sequential([\n",
    "      keras.layers.Input(shape=(NUMBER_FEATURES,)),\n",
    "      keras.layers.Dense(units=NUMBER_TARGETS),\n",
    "      keras.layers.Softmax(),\n",
    "    ])\n",
    "    return model_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use any model with TFF, it needs to be wrapped in an instance of the [`tff.learning.Model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/Model) interface, which exposes methods to stamp the model's forward pass, metadata properties, etc, and also introduces additional elements such as ways to control the process of computing federated metrics. \n",
    "\n",
    "Once you have a Keras model like the one we've just defined above, you can have TFF wrap it for you by invoking [`tff.learning.from_keras_model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/from_keras_model), passing the model and a sample data batch as arguments, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeras_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdummy_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Builds a `tff.learning.Model` for an example mini batch.\n",
       "\n",
       "Args:\n",
       "  keras_model: A `tf.keras.Model` object that is not compiled.\n",
       "  dummy_batch: A nested structure of values that are convertible to *batched*\n",
       "    tensors with the same shapes and types as would be input to `keras_model`.\n",
       "    The values of the tensors are not important and can be filled with any\n",
       "    reasonable input value.\n",
       "  loss: A callable that takes two batched tensor parameters, `y_true` and\n",
       "    `y_pred`, and returns the loss. If the model has multiple outputs, you can\n",
       "    use a different loss on each output by passing a dictionary or a list of\n",
       "    losses. The loss value that will be minimized by the model will then be\n",
       "    the sum of all individual losses, each weighted by `loss_weights`.\n",
       "  loss_weights: (Optional) a list or dictionary specifying scalar coefficients\n",
       "    (Python floats) to weight the loss contributions of different model\n",
       "    outputs. The loss value that will be minimized by the model will then be\n",
       "    the *weighted sum* of all individual losses, weighted by the\n",
       "    `loss_weights` coefficients. If a list, it is expected to have a 1:1\n",
       "      mapping to the model's outputs. If a tensor, it is expected to map\n",
       "      output names (strings) to scalar coefficients.\n",
       "  metrics: (Optional) a list of `tf.keras.metrics.Metric` objects.\n",
       "  optimizer: (Optional) a `tf.keras.optimizer.Optimizer`. If None, returned\n",
       "    model cannot be used for training.\n",
       "\n",
       "Returns:\n",
       "  A `tff.learning.Model` object.\n",
       "\n",
       "Raises:\n",
       "  TypeError: If `keras_model` is not an instance of `tf.keras.Model`.\n",
       "  ValueError: If `keras_model` was compiled.\n",
       "  KeyError: If `loss` is a `dict` and does not have the same keys as\n",
       "    `keras_model.outputs`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/learning/keras_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tff.learning.from_keras_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tff_model_fn() -> tff.learning.Model:\n",
    "    keras_model = create_keras_model_fn()\n",
    "    dummy_batch = (tf.constant(0.0, shape=(TRAINING_BATCH_SIZE, NUMBER_FEATURES), dtype=tf.float32),\n",
    "                   tf.constant(0, shape=(TRAINING_BATCH_SIZE, 1), dtype=tf.int32))\n",
    "    loss_fn = (keras.losses\n",
    "                    .SparseCategoricalCrossentropy())\n",
    "    metrics = [\n",
    "        keras.metrics.SparseCategoricalAccuracy()\n",
    "    ]\n",
    "    tff_model_fn = (tff.learning\n",
    "                       .from_keras_model(keras_model, dummy_batch, loss_fn, None, metrics))\n",
    "    return tff_model_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, since our model will need to be replicated on each of the client devices we wrap the model in a no-argument Python function, a representation of which, will eventually be invoked on each client to create the model on that client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on federated data\n",
    "\n",
    "Now that we have a model wrapped as `tff.learning.Model` for use with TFF, we can let TFF construct a Federated Averaging algorithm by invoking the helper function `tff.learning.build_federated_averaging_process` as follows.\n",
    "\n",
    "Keep in mind that the argument needs to be a constructor (such as `create_tff_model_fn` above), not an already-constructed instance, so that the construction of your model can happen in a context controlled by TFF.\n",
    "\n",
    "One critical note on the Federated Averaging algorithm below, there are 2 optimizers: a \n",
    "\n",
    "1. `client_optimizer_fn` which is only used to compute local model updates on each client. \n",
    "2. `server_optimizer_fn` applies the averaged update to the global model on the server. \n",
    "\n",
    "N.B. the choice of optimizer and learning rate may need to be different than those you would use to train the model on a standard i.i.d. dataset. Start with stochastic gradient descent with a smaller (than normal) learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_federated_averaging_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclient_optimizer_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizerV2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mserver_optimizer_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizerV2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x1132e9ef0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclient_weight_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstateful_delta_aggregate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstateful_model_broadcast_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtensorflow_federated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterative_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterativeProcess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Builds the TFF computations for optimization using federated averaging.\n",
       "\n",
       "Args:\n",
       "  model_fn: A no-arg function that returns a `tff.learning.Model`.\n",
       "  client_optimizer_fn: An optional no-arg callable that returns a\n",
       "    `tf.keras.Optimizer`\n",
       "  server_optimizer_fn: A no-arg callable that returns a `tf.keras.Optimizer`.\n",
       "    The `apply_gradients` method of this optimizer is used to apply client\n",
       "    updates to the server model. The default creates a\n",
       "    `tf.keras.optimizers.SGD` with a learning rate of 1.0, which simply adds\n",
       "    the average client delta to the server's model.\n",
       "  client_weight_fn: Optional function that takes the output of\n",
       "    `model.report_local_outputs` and returns a tensor that provides the weight\n",
       "    in the federated average of model deltas. If not provided, the default is\n",
       "    the total number of examples processed on device.\n",
       "  stateful_delta_aggregate_fn: A `tff.utils.StatefulAggregateFn` where the\n",
       "    `next_fn` performs a federated aggregation and upates state. That is, it\n",
       "    has TFF type `(state@SERVER, value@CLIENTS, weights@CLIENTS) ->\n",
       "    (state@SERVER, aggregate@SERVER)`, where the `value` type is\n",
       "    `tff.learning.framework.ModelWeights.trainable` corresponding to the\n",
       "    object returned by `model_fn`. By default performs arithmetic mean\n",
       "    aggregation, weighted by `client_weight_fn`.\n",
       "  stateful_model_broadcast_fn: A `tff.utils.StatefulBroadcastFn` where the\n",
       "    `next_fn` performs a federated broadcast and upates state. That is, it has\n",
       "    TFF type `(state@SERVER, value@SERVER) -> (state@SERVER, value@CLIENTS)`,\n",
       "    where the `value` type is `tff.learning.framework.ModelWeights`\n",
       "    corresponding to the object returned by `model_fn`. By default performs\n",
       "    identity broadcast.\n",
       "\n",
       "Returns:\n",
       "  A `tff.utils.IterativeProcess`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/learning/federated_averaging.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tff.learning.build_federated_averaging_process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    }
   ],
   "source": [
    "CLIENT_LEARNING_RATE = 1e-2\n",
    "SERVER_LEARNING_RATE = 1e0\n",
    "\n",
    "\n",
    "def create_client_optimizer(learning_rate: float = CLIENT_LEARNING_RATE,\n",
    "                            momentum: float = 0.0,\n",
    "                            nesterov: bool = False) -> keras.optimizers.Optimizer:\n",
    "    client_optimizer = (keras.optimizers\n",
    "                             .SGD(learning_rate, momentum, nesterov))\n",
    "    return client_optimizer\n",
    "\n",
    "\n",
    "def create_server_optimizer(learning_rate: float = SERVER_LEARNING_RATE,\n",
    "                            momentum: float = 0.0,\n",
    "                            nesterov: bool = False) -> keras.optimizers.Optimizer:\n",
    "    server_optimizer = (keras.optimizers\n",
    "                             .SGD(learning_rate, momentum, nesterov))\n",
    "    return server_optimizer\n",
    "\n",
    "\n",
    "federated_averaging_process = (tff.learning\n",
    "                                  .build_federated_averaging_process(create_tff_model_fn, \n",
    "                                                                     create_client_optimizer,\n",
    "                                                                     create_server_optimizer,\n",
    "                                                                     client_weight_fn=None,\n",
    "                                                                     stateful_delta_aggregate_fn=None,\n",
    "                                                                     stateful_model_broadcast_fn=None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened? TFF has constructed a pair of *federated computations* (i.e., programs in TFF's internal glue language) and packaged them into a [`tff.utils.IterativeProcess`](https://www.tensorflow.org/federated/api_docs/python/tff/utils/IterativeProcess) in which these computations are available as a pair of properties `initialize` and `next`.\n",
    "\n",
    "It is a goal of TFF to define computations in a way that they could be executed in real federated learning settings, but currently only local execution simulation runtime is implemented. To execute a computation in a simulator, you simply invoke it like a Python function. This default interpreted environment is not designed for high performance, but it will suffice for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## `initialize`\n",
    "\n",
    "A function that takes no arguments and returns the state of the federated averaging process on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( -> <model=<trainable=<float32[784,10],float32[10]>,non_trainable=<>>,optimizer_state=<int64>,delta_aggregate_state=<>,model_broadcast_state=<>>@SERVER)\n"
     ]
    }
   ],
   "source": [
    "# () -> SERVER_STATE\n",
    "print(federated_averaging_process.initialize.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = federated_averaging_process.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `next`\n",
    "\n",
    "A function that takes current server state and federated data as arguments and returns the updated server state as well as any training metrics. Calling `next` performs a single round of federated averaging consisting of the following steps.\n",
    "\n",
    "1. pushing the server state (including the model parameters) to the clients\n",
    "2. on-device training on their local data\n",
    "3. collecting and averaging model updates\n",
    "4. producing a new updated model at the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 1, metrics: <sparse_categorical_accuracy=0.15695488452911377,loss=2.305616617202759,keras_training_time_client_sum_sec=0.0>\n"
     ]
    }
   ],
   "source": [
    "# extract the training datasets from the federated data\n",
    "federated_training_data = [training_dataset for _, (training_dataset, _) in federated_data.items()]\n",
    "\n",
    "# SERVER_STATE, FEDERATED_DATA -> SERVER_STATE, TRAINING_METRICS\n",
    "state, metrics = federated_averaging_process.next(state, federated_training_data)\n",
    "print(f\"round: 1, metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a few more rounds on the same training data (which will over-fit to a particular set of clients but will converge faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round:2, metrics:<sparse_categorical_accuracy=0.13909775018692017,loss=2.3042614459991455,keras_training_time_client_sum_sec=0.0>\n",
      "round:3, metrics:<sparse_categorical_accuracy=0.13909775018692017,loss=2.303712844848633,keras_training_time_client_sum_sec=0.0>\n",
      "round:4, metrics:<sparse_categorical_accuracy=0.15758144855499268,loss=2.282001495361328,keras_training_time_client_sum_sec=0.0>\n",
      "round:5, metrics:<sparse_categorical_accuracy=0.16290727257728577,loss=2.2804927825927734,keras_training_time_client_sum_sec=0.0>\n",
      "round:6, metrics:<sparse_categorical_accuracy=0.18264411389827728,loss=2.27081036567688,keras_training_time_client_sum_sec=0.0>\n",
      "round:7, metrics:<sparse_categorical_accuracy=0.1791979968547821,loss=2.266880989074707,keras_training_time_client_sum_sec=0.0>\n",
      "round:8, metrics:<sparse_categorical_accuracy=0.15726816654205322,loss=2.26709246635437,keras_training_time_client_sum_sec=0.0>\n",
      "round:9, metrics:<sparse_categorical_accuracy=0.185776948928833,loss=2.250056505203247,keras_training_time_client_sum_sec=0.0>\n",
      "round:10, metrics:<sparse_categorical_accuracy=0.18421052396297455,loss=2.247758626937866,keras_training_time_client_sum_sec=0.0>\n"
     ]
    }
   ],
   "source": [
    "NUMBER_ROUNDS = 10\n",
    "\n",
    "for n in range(2, NUMBER_ROUNDS + 1):\n",
    "    state, metrics = federated_averaging_process.next(state, federated_training_data)\n",
    "    print(f\"round:{n}, metrics:{metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a proper training simulation\n",
    "\n",
    "A proper federated learning simulation would randomly sample new clients for each training round, allow for evaluation of training progress on training and testing data, and log training and testing metrics to TensorBoard for reference.\n",
    "\n",
    "Here we define a function that randomly sample new clients prior to each training round and logs training metrics TensorBoard. We defer handling testing data until we discuss evaluation at the end of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_federated_averaging(federated_averaging_process: tff.utils.IterativeProcess,\n",
    "                                 training_source: tff.simulation.ClientData,\n",
    "                                 testing_source: tff.simulation.ClientData,\n",
    "                                 sample_size: typing.Union[float, int],\n",
    "                                 random_state: np.random.RandomState,\n",
    "                                 number_rounds: int,\n",
    "                                 initial_state: None = None,\n",
    "                                 tensorboard_logging_dir: str = None):\n",
    "    \n",
    "    state = federated_averaging_process.initialize() if initial_state is None else initial_state\n",
    "    \n",
    "    if tensorboard_logging_dir is not None:\n",
    "        \n",
    "        if not os.path.isdir(tensorboard_logging_dir):\n",
    "            os.makedirs(tensorboard_logging_dir)\n",
    "\n",
    "        summary_writer = (tf.summary\n",
    "                            .create_file_writer(tensorboard_logging_dir))\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            for n in range(number_rounds):\n",
    "                federated_data = create_federated_data(training_source,\n",
    "                                                       testing_source,\n",
    "                                                       sample_size,\n",
    "                                                       random_state)\n",
    "                anonymized_training_data = [dataset for _, (dataset, _) in federated_data.items()]\n",
    "                state, metrics = federated_averaging_process.next(state, anonymized_training_data)\n",
    "                print(f\"Round: {n}, Training metrics: {metrics}\")\n",
    "\n",
    "                for name, value in metrics._asdict().items():\n",
    "                    tf.summary.scalar(name, value, step=n)          \n",
    "    else:\n",
    "        for n in range(number_rounds):\n",
    "            federated_data = create_federated_data(training_source,\n",
    "                                                   testing_source,\n",
    "                                                   sample_size,\n",
    "                                                   random_state)\n",
    "            anonymized_training_data = [dataset for _, (dataset, _) in federated_data.items()]\n",
    "            state, metrics = federated_averaging_process.next(state, anonymized_training_data)\n",
    "            print(f\"Round: {n}, Training metrics: {metrics}\")\n",
    "    \n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "/Users/pughdr/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:64: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0, Metrics: <sparse_categorical_accuracy=0.09774436056613922,loss=2.7396252155303955,keras_training_time_client_sum_sec=0.0>\n",
      "Round: 1, Metrics: <sparse_categorical_accuracy=0.12916666269302368,loss=2.3470051288604736,keras_training_time_client_sum_sec=0.0>\n",
      "Round: 2, Metrics: <sparse_categorical_accuracy=0.12727834284305573,loss=2.3454086780548096,keras_training_time_client_sum_sec=0.0>\n",
      "Round: 3, Metrics: <sparse_categorical_accuracy=0.14518563449382782,loss=2.337625503540039,keras_training_time_client_sum_sec=0.0>\n",
      "Round: 4, Metrics: <sparse_categorical_accuracy=0.12811826169490814,loss=2.3283214569091797,keras_training_time_client_sum_sec=0.0>\n"
     ]
    }
   ],
   "source": [
    "federated_averaging_process = (tff.learning\n",
    "                                  .build_federated_averaging_process(create_tff_model_fn, \n",
    "                                                                     create_client_optimizer,\n",
    "                                                                     create_server_optimizer,\n",
    "                                                                     client_weight_fn=None,\n",
    "                                                                     stateful_delta_aggregate_fn=None,\n",
    "                                                                     stateful_model_broadcast_fn=None))\n",
    "_random_state = np.random.RandomState(42)\n",
    "_tensorboard_logging_dir = \"../results/logs/tensorboard\"\n",
    "updated_state, current_metrics = simulate_federated_averaging(federated_averaging_process,\n",
    "                                                              training_source=emnist_train,\n",
    "                                                              testing_source=emnist_test,\n",
    "                                                              sample_size=0.01,\n",
    "                                                              random_state=_random_state,\n",
    "                                                              number_rounds=5,\n",
    "                                                              tensorboard_logging_dir=_tensorboard_logging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnonymousTuple([('model', AnonymousTuple([('trainable', AnonymousTuple([(None, array([[-0.0066374 , -0.07162341,  0.07334219, ...,  0.00527187,\n",
       "         0.05876873, -0.00786667],\n",
       "       [-0.04240225,  0.05663963,  0.07935971, ...,  0.02865732,\n",
       "        -0.03983072, -0.04299481],\n",
       "       [-0.06109829,  0.07948509, -0.06211761, ...,  0.04607752,\n",
       "         0.03252228,  0.01537533],\n",
       "       ...,\n",
       "       [-0.07103737, -0.05495797, -0.00063683, ..., -0.06484114,\n",
       "        -0.02665211,  0.00245448],\n",
       "       [-0.07363674,  0.03300316, -0.05461916, ..., -0.03586686,\n",
       "         0.03189842,  0.03804823],\n",
       "       [-0.02130092,  0.07461053,  0.02983751, ...,  0.07482325,\n",
       "         0.04991261, -0.03923729]], dtype=float32)), (None, array([-0.0013023 , -0.00033977,  0.00047533, -0.00087597,  0.00015358,\n",
       "        0.00108823,  0.00442215,  0.00117538, -0.00171131, -0.00308531],\n",
       "      dtype=float32))])), ('non_trainable', AnonymousTuple([]))])), ('optimizer_state', AnonymousTuple([(None, 5)])), ('delta_aggregate_state', AnonymousTuple([])), ('model_broadcast_state', AnonymousTuple([]))])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnonymousTuple([('sparse_categorical_accuracy', 0.12811826), ('loss', 2.3283215), ('keras_training_time_client_sum_sec', 0.0)])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing the model implementation\n",
    "\n",
    "Keras is the recommended high-level model API for TensorFlow and you should be using Keras models and creating TFF models using [`tff.learning.from_keras_model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/from_keras_model) whenever possible.\n",
    "\n",
    "However, [`tff.learning`](https://www.tensorflow.org/federated/api_docs/python/tff/learning) provides a lower-level model interface, [`tff.learning.Model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/Model), that exposes the minimal functionality necessary for using a model for federated learning. Directly implementing this interface (possibly still using building blocks from [`keras`](https://www.tensorflow.org/guide/keras)) allows for maximum customization without modifying the internals of the federated learning algorithms.\n",
    "\n",
    "Now we are going to repeat the above from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model variables\n",
    "\n",
    "We start by defining a new Python class that inherits from `tff.learning.Model`. In the class constructor (i.e., the `__init__` method) we will initialize all relevant variables using TF primatives as well as define the our \"input spec\" which defines the shape and types of the tensors that will hold input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tff.learning.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # initialize some trainable variables\n",
    "        self._weights = tf.Variable(\n",
    "            initial_value=lambda: tf.zeros(dtype=tf.float32, shape=(NUMBER_FEATURES, NUMBER_TARGETS)),\n",
    "            name=\"weights\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self._bias = tf.Variable(\n",
    "            initial_value=lambda: tf.zeros(dtype=tf.float32, shape=(NUMBER_TARGETS,)),\n",
    "            name=\"bias\",\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        # initialize some variables used in computing metrics\n",
    "        self._number_examples = tf.Variable(0.0, name='number_examples', trainable=False)\n",
    "        self._total_loss = tf.Variable(0.0, name='total_loss', trainable=False)\n",
    "        self._number_true_positives = tf.Variable(0.0, name='number_true_positives', trainable=False)\n",
    "        \n",
    "        # define the input spec\n",
    "        self._input_spec = collections.OrderedDict([\n",
    "            ('X', tf.TensorSpec([None, NUMBER_FEATURES], tf.float32)),\n",
    "            ('y', tf.TensorSpec([None, 1], tf.int32))\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def input_spec(self):\n",
    "        return self._input_spec\n",
    "    \n",
    "    @property\n",
    "    def local_variables(self):\n",
    "        return [self._number_examples, self._total_loss, self._number_true_positives]\n",
    "\n",
    "    @property\n",
    "    def non_trainable_variables(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return [self._weights, self._bias]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the forward pass\n",
    "\n",
    "With the variables for model parameters and cumulative statistics in place we can now define the `forward_pass` method that computes loss, makes predictions, and updates the cumulative statistics for a single batch of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tff.learning.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # initialize some trainable variables\n",
    "        self._weights = tf.Variable(\n",
    "            initial_value=lambda: tf.zeros(dtype=tf.float32, shape=(NUMBER_FEATURES, NUMBER_TARGETS)),\n",
    "            name=\"weights\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self._bias = tf.Variable(\n",
    "            initial_value=lambda: tf.zeros(dtype=tf.float32, shape=(NUMBER_TARGETS,)),\n",
    "            name=\"bias\",\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        # initialize some variables used in computing metrics\n",
    "        self._number_examples = tf.Variable(0.0, name='number_examples', trainable=False)\n",
    "        self._total_loss = tf.Variable(0.0, name='total_loss', trainable=False)\n",
    "        self._number_true_positives = tf.Variable(0.0, name='number_true_positives', trainable=False)\n",
    "        \n",
    "        # define the input spec\n",
    "        self._input_spec = collections.OrderedDict([\n",
    "            ('X', tf.TensorSpec([None, NUMBER_FEATURES], tf.float32)),\n",
    "            ('y', tf.TensorSpec([None, 1], tf.int32))\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def input_spec(self):\n",
    "        return self._input_spec\n",
    "    \n",
    "    @property\n",
    "    def local_variables(self):\n",
    "        return [self._number_examples, self._total_loss, self._number_true_positives]\n",
    "\n",
    "    @property\n",
    "    def non_trainable_variables(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return [self._weights, self._bias]\n",
    "\n",
    "    @tf.function\n",
    "    def _count_true_positives(self, y_true, y_pred):\n",
    "        return tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "\n",
    "    @tf.function\n",
    "    def _linear_transformation(self, batch):\n",
    "        X = batch['X']\n",
    "        W, b = self.trainable_variables\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        return Z\n",
    "    \n",
    "    @tf.function\n",
    "    def _loss_fn(self, y_true, probabilities):\n",
    "        return -tf.reduce_mean(tf.reduce_sum(tf.one_hot(y_true, NUMBER_TARGETS) * tf.math.log(probabilities), axis=1))\n",
    "    \n",
    "    @tf.function\n",
    "    def _model_fn(self, batch):\n",
    "        Z = self._linear_transformation(batch)\n",
    "        probabilities = tf.nn.softmax(Z)\n",
    "        return probabilities\n",
    "    \n",
    "    @tf.function\n",
    "    def forward_pass(self, batch, training=True):\n",
    "        probabilities = self._model_fn(batch)\n",
    "        y_pred = tf.argmax(probabilities, axis=1, output_type=tf.int32)\n",
    "        y_true = tf.reshape(batch['y'], shape=[-1])\n",
    "\n",
    "        # compute local variables\n",
    "        loss = self._loss_fn(y_true, probabilities)\n",
    "        true_positives = self._count_true_positives(y_true, y_pred)\n",
    "        number_examples = tf.size(y_true, out_type=tf.float32)\n",
    "        \n",
    "        # update local variables\n",
    "        self._total_loss.assign_add(loss)\n",
    "        self._number_true_positives.assign_add(true_positives)\n",
    "        self._number_examples.assign_add(number_examples)\n",
    "\n",
    "        batch_output = tff.learning.BatchOutput(\n",
    "            loss=loss,\n",
    "            predictions=y_pred,\n",
    "            num_examples=tf.cast(number_examples, tf.int32)\n",
    "        )\n",
    "        return batch_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the local metrics\n",
    "\n",
    "Next, we define a method `report_local_outputs` that returns a set of local metrics. These are the values, in addition to model updates (which are handled automatically), that are eligible to be aggregated to the server in a federated learning or evaluation process.\n",
    "\n",
    "Finally, we need to determine how to aggregate the local metrics emitted by each device by defining `federated_output_computation`. This is the only part of the code that isn't written in TensorFlow - it's a federated computation expressed in TFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tff.learning.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # initialize some trainable variables\n",
    "        self._weights = tf.Variable(\n",
    "            initial_value=lambda: tf.zeros(dtype=tf.float32, shape=(NUMBER_FEATURES, NUMBER_TARGETS)),\n",
    "            name=\"weights\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self._bias = tf.Variable(\n",
    "            initial_value=lambda: tf.zeros(dtype=tf.float32, shape=(NUMBER_TARGETS,)),\n",
    "            name=\"bias\",\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        # initialize some variables used in computing metrics\n",
    "        self._number_examples = tf.Variable(0.0, name='number_examples', trainable=False)\n",
    "        self._total_loss = tf.Variable(0.0, name='total_loss', trainable=False)\n",
    "        self._number_true_positives = tf.Variable(0.0, name='number_true_positives', trainable=False)\n",
    "        \n",
    "        # define the input spec\n",
    "        self._input_spec = collections.OrderedDict([\n",
    "            ('X', tf.TensorSpec([None, NUMBER_FEATURES], tf.float32)),\n",
    "            ('y', tf.TensorSpec([None, 1], tf.int32))\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def federated_output_computation(self):\n",
    "        return self._aggregate_metrics_across_clients\n",
    "    \n",
    "    @property\n",
    "    def input_spec(self):\n",
    "        return self._input_spec\n",
    "    \n",
    "    @property\n",
    "    def local_variables(self):\n",
    "        return [self._number_examples, self._total_loss, self._number_true_positives]\n",
    "\n",
    "    @property\n",
    "    def non_trainable_variables(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return [self._weights, self._bias]\n",
    "    \n",
    "    @tff.federated_computation\n",
    "    def _aggregate_metrics_across_clients(metrics):\n",
    "        aggregated_metrics = {\n",
    "            'number_examples': tff.federated_sum(metrics.number_examples),\n",
    "            'average_loss': tff.federated_mean(metrics.average_loss, metrics.number_examples),\n",
    "            'accuracy': tff.federated_mean(metrics.accuracy, metrics.number_examples)\n",
    "        }\n",
    "        return aggregated_metrics\n",
    "\n",
    "    @tf.function\n",
    "    def _count_true_positives(self, y_true, y_pred):\n",
    "        return tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "\n",
    "    @tf.function\n",
    "    def _linear_transformation(self, batch):\n",
    "        X = batch['X']\n",
    "        W, b = self.trainable_variables\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        return Z\n",
    "    \n",
    "    @tf.function\n",
    "    def _loss_fn(self, y_true, probabilities):\n",
    "        return -tf.reduce_mean(tf.reduce_sum(tf.one_hot(y_true, NUMBER_TARGETS) * tf.math.log(probabilities), axis=1))\n",
    "    \n",
    "    @tf.function\n",
    "    def _model_fn(self, batch):\n",
    "        Z = self._linear_transformation(batch)\n",
    "        probabilities = tf.nn.softmax(Z)\n",
    "        return probabilities\n",
    "    \n",
    "    @tf.function\n",
    "    def forward_pass(self, batch, training=True):\n",
    "        probabilities = self._model_fn(batch)\n",
    "        y_pred = tf.argmax(probabilities, axis=1, output_type=tf.int32)\n",
    "        y_true = tf.reshape(batch['y'], shape=[-1])\n",
    "\n",
    "        # compute local variables\n",
    "        loss = self._loss_fn(y_true, probabilities)\n",
    "        true_positives = self._count_true_positives(y_true, y_pred)\n",
    "        number_examples = tf.cast(tf.size(y_true), tf.float32)\n",
    "        \n",
    "        # update local variables\n",
    "        self._total_loss.assign_add(loss)\n",
    "        self._number_true_positives.assign_add(true_positives)\n",
    "        self._number_examples.assign_add(number_examples)\n",
    "\n",
    "        batch_output = tff.learning.BatchOutput(\n",
    "            loss=loss,\n",
    "            predictions=y_pred,\n",
    "            num_examples=tf.cast(number_examples, tf.int32)\n",
    "        )\n",
    "        return batch_output\n",
    "\n",
    "    @tf.function\n",
    "    def report_local_outputs(self):\n",
    "        local_metrics = collections.OrderedDict([\n",
    "            ('number_examples', self._number_examples),\n",
    "            ('average_loss', self._total_loss / self._number_examples),\n",
    "            ('accuracy', self._number_true_positives / self._number_examples)\n",
    "        ])\n",
    "        return local_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the abstract methods and properties defined by [`tff.learning.Model`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/Model) corresponds to the code snippets in the preceding section that introduced the variables and defined the loss and statistics.\n",
    "\n",
    "Here are a few points worth highlighting:\n",
    "\n",
    "* All state that your model will use must be captured as TensorFlow variables, as TFF does not use Python at runtime (remember your code should be written such that it can be deployed to mobile devices).\n",
    "* Your model should describe what form of data it accepts (input_spec), as in general, TFF is a strongly-typed environment and wants to determine type signatures for all components. Declaring the format of your model's input is an essential part of it.\n",
    "* Although technically not required, we recommend wrapping all TensorFlow logic (forward pass, metric calculations, etc.) as tf.functions, as this helps ensure the TensorFlow can be serialized, and removes the need for explicit control dependencies.\n",
    "\n",
    "The above is sufficient for evaluation and algorithms like Federated SGD. However, for Federated Averaging, we need to specify how the model should train locally on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTrainableModel(MNISTModel, tff.learning.TrainableModel):\n",
    "\n",
    "    @tf.function\n",
    "    def train_on_batch(self, batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self.forward_pass(batch)\n",
    "        gradients = tape.gradient(output.loss, self.trainable_variables)\n",
    "        optimizer = keras.optimizers.SGD(0.02) # don't hard code this here!\n",
    "        optimizer.apply_gradients(zip(tf.nest.flatten(gradients), tf.nest.flatten(self.trainable_variables)))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating federated training with the new model\n",
    "\n",
    "With all the above in place, the remainder of the process looks like what we've seen already - just replace the model constructor with the constructor of our new model class, and use the two federated computations in the iterative process you created to cycle through training rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pughdr/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/learning/federated_averaging.py:242: UserWarning: tff.learning.build_federated_averaging_process will start requiring a new argument 'client_optimizer_fn'. Specify the local client optimizer here rather than building a ttf.learning.TrainableModel\n",
      "  warnings.warn('tff.learning.build_federated_averaging_process will start '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0, Metrics: <accuracy=0.10087719559669495,average_loss=0.09287919104099274,number_examples=3192.0>\n",
      "Round: 1, Metrics: <accuracy=0.09107142686843872,average_loss=0.0920669287443161,number_examples=3360.0>\n",
      "Round: 2, Metrics: <accuracy=0.09638553857803345,average_loss=0.09503784775733948,number_examples=3237.0>\n",
      "Round: 3, Metrics: <accuracy=0.10473890602588654,average_loss=0.0915011316537857,number_examples=3313.0>\n",
      "Round: 4, Metrics: <accuracy=0.10224822908639908,average_loss=0.09469816833734512,number_examples=3247.0>\n"
     ]
    }
   ],
   "source": [
    "federated_averaging_process = (tff.learning\n",
    "                                  .build_federated_averaging_process(MNISTrainableModel))\n",
    "\n",
    "_random_state = np.random.RandomState(42)\n",
    "updated_state, current_metrics = simulate_federated_averaging(federated_averaging_process,\n",
    "                                                              training_source=emnist_train,\n",
    "                                                              testing_source=emnist_test,\n",
    "                                                              sample_size=0.01,\n",
    "                                                              random_state=_random_state,\n",
    "                                                              number_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnonymousTuple([('model', AnonymousTuple([('trainable', AnonymousTuple([(None, array([[-1.6500195e-04, -5.2444241e-04, -1.3999402e-04, ...,\n",
       "         9.0673857e-05,  2.3423761e-04, -2.0685904e-04],\n",
       "       [-1.6500195e-04, -5.2444241e-04, -1.3999402e-04, ...,\n",
       "         9.0673857e-05,  2.3423761e-04, -2.0685904e-04],\n",
       "       [-1.6500195e-04, -5.2444241e-04, -1.3999402e-04, ...,\n",
       "         9.0673857e-05,  2.3423761e-04, -2.0685904e-04],\n",
       "       ...,\n",
       "       [-1.6500195e-04, -5.2444241e-04, -1.3999402e-04, ...,\n",
       "         9.0673857e-05,  2.3423761e-04, -2.0685904e-04],\n",
       "       [-1.6500178e-04, -5.2444229e-04, -1.3999399e-04, ...,\n",
       "         9.0673624e-05,  2.3423752e-04, -2.0685914e-04],\n",
       "       [-1.6500178e-04, -5.2444229e-04, -1.3999399e-04, ...,\n",
       "         9.0673624e-05,  2.3423752e-04, -2.0685914e-04]], dtype=float32)), (None, array([-1.6500195e-04, -5.2444241e-04, -1.3999402e-04, -4.1510188e-04,\n",
       "        8.9941721e-05,  8.3740143e-04,  1.9914712e-04,  9.0673857e-05,\n",
       "        2.3423761e-04, -2.0685904e-04], dtype=float32))])), ('non_trainable', AnonymousTuple([]))])), ('optimizer_state', AnonymousTuple([(None, 5)])), ('delta_aggregate_state', AnonymousTuple([])), ('model_broadcast_state', AnonymousTuple([]))])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnonymousTuple([('accuracy', 0.103788115), ('average_loss', 0.09449896), ('number_examples', 3247.0)])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "All of our experiments so far presented only federated training metrics - the average metrics over all batches of data trained across all clients in the round. This introduces the normal concerns about overfitting, especially since we used the same set of clients on each round for simplicity, but there is an additional notion of overfitting in training metrics specific to the Federated Averaging algorithm. This is easiest to see if we imagine each client had a single batch of data, and we train on that batch for many iterations (epochs). In this case, the local model will quickly exactly fit to that one batch, and so the local accuracy metric we average will approach 1.0. Thus, these training metrics can be taken as a sign that training is progressing, but not much more.\n",
    "\n",
    "Two potential sources of overfitting: overfitting the shared model, overfitting the local models.\n",
    "\n",
    "To perform evaluation on federated data, you can construct another federated computation designed for just this purpose, using the [`tff.learning.build_federated_evaluation`](https://www.tensorflow.org/federated/api_docs/python/tff/learning/build_federated_evaluation) function, and passing in your model constructor as an argument. \n",
    "\n",
    "Note that evaluation doesn't perform gradient descent and there's no need to construct optimizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_federated_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Builds the TFF computation for federated evaluation of the given model.\n",
       "\n",
       "Args:\n",
       "  model_fn: A no-argument function that returns a `tff.learning.Model`.\n",
       "\n",
       "Returns:\n",
       "  A federated computation (an instance of `tff.Computation`) that accepts\n",
       "  model parameters and federated data, and returns the evaluation metrics\n",
       "  as aggregated by `tff.learning.Model.federated_output_computation`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/learning/federated_evaluation.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tff.learning.build_federated_evaluation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = (tff.learning\n",
    "                 .build_federated_evaluation(create_tff_model_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_federated.python.core.impl.computation_impl.ComputationImpl"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<<trainable=<float32[784,10],float32[10]>,non_trainable=<>>@SERVER,{<float32[?,784],int32[?,1]>*}@CLIENTS> -> <sparse_categorical_accuracy=float32@SERVER,loss=float32@SERVER,keras_training_time_client_sum_sec=float32@SERVER>)\n"
     ]
    }
   ],
   "source": [
    "print(evaluation.type_signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SERVER_MODEL, FEDERATED_DATA -> TRAINING_METRICS`\n",
    "\n",
    "No need to be concerned about the details at this point, just be aware that it takes the following general form, similar to `tff.utils.IterativeProcess.next` but with two important differences. \n",
    "\n",
    "1. We are not returning server state, since evaluation doesn't modify the model or any other aspect of state - you can think of it as stateless.\n",
    "2. Evaluation only needs the model and doesn't require any other part of server state that might be associated with training, such as optimizer variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics = evaluation(state.model, federated_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnonymousTuple([('accuracy', 0.59687597), ('loss', 1.8894539), ('num_examples', 33290.0)])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we get. Note the numbers look marginally better than what was reported by the last round of training above. By convention, the training metrics reported by the iterative training process generally reflect the performance of the model at the beginning of the training round, so the evaluation metrics will always be one step ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compile a test sample of federated data and rerun evaluation on the test data. The data will come from the same sample of real users, but from a distinct held-out data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pughdr/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:69: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    }
   ],
   "source": [
    "# resample 1% of all clients\n",
    "client_ids = sample_client_ids(emnist_test.client_ids, 0.01, random_state)\n",
    "federated_testing_data = create_testing_datasets(client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = evaluation(state.model, federated_testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnonymousTuple([('accuracy', 0.6329114), ('loss', 1.8689167), ('num_examples', 395.0)])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_federated_averaging(federated_averaging_process: tff.utils.IterativeProcess,\n",
    "                                 federated_evaluation,\n",
    "                                 training_source: tff.simulation.ClientData,\n",
    "                                 testing_source: tff.simulation.ClientData,\n",
    "                                 sample_size: typing.Union[float, int],\n",
    "                                 random_state: np.random.RandomState,\n",
    "                                 number_rounds: int,\n",
    "                                 initial_state: None = None,\n",
    "                                 tensorboard_logging_dir: str = None):\n",
    "    \n",
    "    state = federated_averaging_process.initialize() if initial_state is None else initial_state\n",
    "    \n",
    "    if tensorboard_logging_dir is not None:\n",
    "        \n",
    "        if not os.path.isdir(tensorboard_logging_dir):\n",
    "            os.makedirs(tensorboard_logging_dir)\n",
    "\n",
    "        summary_writer = (tf.summary\n",
    "                            .create_file_writer(tensorboard_logging_dir))\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            for n in range(number_rounds):\n",
    "                federated_data = create_federated_data(training_source,\n",
    "                                                       testing_source,\n",
    "                                                       sample_size,\n",
    "                                                       random_state)\n",
    "                \n",
    "                # extract the training and testing datasets\n",
    "                anonymized_training_data = []\n",
    "                anonymized_testing_data = []\n",
    "                for training_dataset, testing_dataset in federated_data.values():\n",
    "                    anonymized_training_data.append(training_dataset)\n",
    "                    anonymized_testing_data.append(testing_dataset)\n",
    "        \n",
    "                state, metrics = federated_averaging_process.next(state, anonymized_training_data)\n",
    "                training_metrics = federated_evaluation(state.model, anonymized_training_data)\n",
    "                testing_metrics = federated_evaluation(state.model, anonymized_testing_data)\n",
    "                print(f\"Round: {n}, Training metrics: {training_metrics}, Testing metrics: {testing_metrics}\")\n",
    "\n",
    "                # tensorboard logging\n",
    "                for name, value in training_metrics._asdict().items():\n",
    "                    tf.summary.scalar(name, value, step=n)\n",
    "                \n",
    "                for name, value in testing_metrics._asdict().items():\n",
    "                    tf.summary.scalar(name, value, step=n)\n",
    "    else:\n",
    "        for n in range(number_rounds):\n",
    "            federated_data = create_federated_data(training_source,\n",
    "                                                       testing_source,\n",
    "                                                       sample_size,\n",
    "                                                       random_state)\n",
    "                \n",
    "            # extract the training and testing datasets\n",
    "            anonymized_training_data = []\n",
    "            anonymized_testing_data = []\n",
    "            for training_dataset, testing_dataset in federated_data.values():\n",
    "                anonymized_training_data.append(training_dataset)\n",
    "                anonymized_testing_data.append(testing_dataset)\n",
    "\n",
    "            state, metrics = federated_averaging_process.next(state, anonymized_training_data)\n",
    "            training_metrics = federated_evaluation(state.model, anonymized_training_data)\n",
    "            testing_metrics = federated_evaluation(state.model, anonymized_testing_data)\n",
    "            print(f\"Round: {n}, Training metrics: {training_metrics}, Testing metrics: {testing_metrics}\")\n",
    "    \n",
    "    return state, (training_metrics, testing_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function zero_all_if_any_non_finite at 0x1320f4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "/Users/pughdr/Training/data-science-project-templates/tensorflow-federated-data-science-project/env/lib/python3.7/site-packages/tensorflow_federated/python/simulation/hdf5_client_data.py:64: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  collections.OrderedDict((name, ds.value) for name, ds in sorted(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0, Training metrics: <sparse_categorical_accuracy=0.1541353315114975,loss=2.3019914627075195,keras_training_time_client_sum_sec=0.0>, Testing metrics: <sparse_categorical_accuracy=0.146214097738266,loss=2.3117120265960693,keras_training_time_client_sum_sec=0.0>\n",
      "Round: 1, Training metrics: <sparse_categorical_accuracy=0.12886904180049896,loss=2.286670684814453,keras_training_time_client_sum_sec=0.0>, Testing metrics: <sparse_categorical_accuracy=0.11528822034597397,loss=2.2982778549194336,keras_training_time_client_sum_sec=0.0>\n",
      "Round: 2, Training metrics: <sparse_categorical_accuracy=0.14859437942504883,loss=2.2734010219573975,keras_training_time_client_sum_sec=0.0>, Testing metrics: <sparse_categorical_accuracy=0.12919896841049194,loss=2.284796953201294,keras_training_time_client_sum_sec=0.0>\n",
      "Round: 3, Training metrics: <sparse_categorical_accuracy=0.10141865164041519,loss=2.285041570663452,keras_training_time_client_sum_sec=0.0>, Testing metrics: <sparse_categorical_accuracy=0.09068010002374649,loss=2.3119335174560547,keras_training_time_client_sum_sec=0.0>\n",
      "Round: 4, Training metrics: <sparse_categorical_accuracy=0.17400677502155304,loss=2.25435471534729,keras_training_time_client_sum_sec=0.0>, Testing metrics: <sparse_categorical_accuracy=0.17312660813331604,loss=2.2572340965270996,keras_training_time_client_sum_sec=0.0>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-caa3f3ffdb37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                                               \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                               \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_random_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                                               number_rounds=5)\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "federated_averaging_process = (tff.learning\n",
    "                                  .build_federated_averaging_process(create_tff_model_fn, \n",
    "                                                                     create_client_optimizer,\n",
    "                                                                     create_server_optimizer,\n",
    "                                                                     client_weight_fn=None,\n",
    "                                                                     stateful_delta_aggregate_fn=None,\n",
    "                                                                     stateful_model_broadcast_fn=None))\n",
    "\n",
    "federated_evaluation = (tff.learning\n",
    "                           .build_federated_evaluation(create_tff_model_fn))\n",
    "\n",
    "_random_state = np.random.RandomState(42)\n",
    "updated_state, current_metrics = simulate_federated_averaging(federated_averaging_process,\n",
    "                                                              federated_evaluation,\n",
    "                                                              training_source=emnist_train,\n",
    "                                                              testing_source=emnist_test,\n",
    "                                                              sample_size=0.01,\n",
    "                                                              random_state=_random_state,\n",
    "                                                              number_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
